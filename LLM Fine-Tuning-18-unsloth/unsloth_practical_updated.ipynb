{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtXGqeLLxDjU",
    "outputId": "5295d237-c210-41c0-f860-dd42b6c36909"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wksaDdY1JeID"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -U unsloth unsloth_zoo\n",
    "!pip install transformers==4.56.2\n",
    "!pip install trl==0.22.2 peft accelerate datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aeY2ZWHxElU"
   },
   "outputs": [],
   "source": [
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    "#     max_seq_length=4096,\n",
    "#     dtype=None,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=32,\n",
    "#     target_modules=[...],\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=\"unsloth\"\n",
    "# )\n",
    "\n",
    "# alpaca_prompt = \"\"\"...\"\"\"\n",
    "# EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_dataset=dataset,\n",
    "#     packing=True,\n",
    "#     args=SFTConfig(\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         num_train_epochs=1,\n",
    "#         learning_rate=2e-5,\n",
    "#         optim=\"adamw_8bit\",\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# torch.cuda.max_memory_reserved()\n",
    "# FastLanguageModel.for_inference(model)\n",
    "# model.save_pretrained(\"lora_model\")\n",
    "# tokenizer.save_pretrained(\"lora_model\")\n",
    "# FastLanguageModel.from_pretrained(\"lora_model\")\n",
    "# model.save_pretrained_merged(..., save_method=\"merged_16bit\")\n",
    "# model.save_pretrained_gguf(..., quantization_method=\"q4_k_m\")\n",
    "# model.to(\"cuda\")\n",
    "# seed=3407\n",
    "# max_steps = 100\n",
    "# num_train_epochs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHl8b3JBxdzp",
    "outputId": "3ff43d7b-4621-4e11-f0b6-9a4c12d97c97"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Imports & Config\n",
    "# =========================\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 3407\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Faster & stable matmul on NVIDIA GPUs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# GPU sanity check\n",
    "assert torch.cuda.is_available(), \"❌ Please enable GPU runtime (Colab → Runtime → GPU)\"\n",
    "\n",
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# =========================\n",
    "# Global Config\n",
    "# =========================\n",
    "max_seq_length = 4096        # Demonstrates long-context + RoPE scaling\n",
    "dtype = None                # Auto-detect (FP16 on T4, BF16 on A100/L4)\n",
    "load_in_4bit = True         # Memory-efficient QLoRA-style loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "7144a8faf2d84e8c92248f028e847989",
      "f2ecdca3ccb24cb596d76dd2c0d359c9",
      "ec20fccc6b5b4b8d89d140d35a38b519",
      "aa4633637fcf49e7b0056714321ee01e",
      "73b660c832664ad78551f6671032bc3a",
      "d2444b94e7984fcfa558ae49b74d5af5",
      "8eedb5ad73fc434a8f87b817c3146441",
      "2ec2cd4af4a04a1b8e73557759261bae",
      "719dfeee48864b3f80ba4ac9fdf82a08",
      "68df8819ab324d00bbbdc36fbcaf246e",
      "37092035c73944d683c58b1c43cfe1c1",
      "4c7bb6516ad542a59b28e8e03e510e41",
      "59cecfa3ff35413881ee08b25c02eff8",
      "6410cb35ccaf46549c55e898bd72743d",
      "0e9b3b9a8de7413c84be8eee72fe9dc0",
      "2d985eaadd41491b98708fb49c6ea8e2",
      "4358a99e308a4330aa6570ff475e9618",
      "992e413d69f04cd7b93471c9938e64e5",
      "5287c517157a4c508747f4800f69ac56",
      "4f6c8707f8fc49b1a365ddd6171b3964",
      "05a821edfaf149df9ca1bc8076d23b9d",
      "dc4e66c1104e4778b95c18a27608a212",
      "34a6274b693046b5875981459b5b6f93",
      "47caa4cd72764ae2a7806b1793f0a48a",
      "eb2a9d32378d44cebe1cd9f986902e3a",
      "eea39ecf32ad46ae978981992146c89a",
      "27573bc8889847329cc5e6267e3cb317",
      "6707e2f6393d4a8d9acd4e97a2c333bf",
      "31ebc4926e8e457c94b3645a9a4cf1c7",
      "c8895150cdc94cf5a1b5dbd1305930dd",
      "67bca6a7356a401aa1c5128523ca5c6c",
      "1acfb7089e7443a0be18c636d4b2e003",
      "2bcc8782c0f64216b63d9945b305aa39",
      "ff3695e0a3984b1a83da07b4d08934c3",
      "07e81bd010c348548cac7fdc26917d03",
      "aacbd01a081144ce8759e67505e3d154",
      "4b7e2016ed074a8389ad38223fd6c12c",
      "e1bd34d525d94a95badff0c3abe926e3",
      "2b3ebe21faea43caa92434830fd65771",
      "86bb502af2eb4b39bdca841c60f20ed4",
      "9943f7f6be894c6d83091aaf08cf732e",
      "81270769eb5d4b49add561bf08989c26",
      "f535c8e9bb5b4d38b01e21359cffda95",
      "af734d07b393463a9b2a6eb05d7c5bf8",
      "429aa38fcefa44b893dc6357133c6d41",
      "bcb5467f31194d52bb7b2035840653fb",
      "03445cd0c19d4a8db8a4efbabf9163d0",
      "bf88cc7579a94aa1b81419ad782c2180",
      "a376d29f7c1545aa8dcd988b7030437b",
      "c57ae87e762a4af49ae32aec83585d8f",
      "a3ff797694c5441180b31776e302ab6c",
      "1cd72b85312041d79c8ce865456b38a7",
      "a1625b89e6ea4e3eb0268633f512e984",
      "6864257afd5a4a82b5adf3b60c3a32f1",
      "56c53131408b486b840aa70220d787f6",
      "033fd9927cfc4a8c9656c3f272886dd0",
      "ee54b9d29497469a85359e9d1013467d",
      "07fcc180f0244c918a3280a525b42104",
      "e10daaff13364c8a8e709d79aac2d497",
      "7534dbc866cd4350a7fb520cea50f01e",
      "bdc6bbb3bfe74fd3947f49d523b93fde",
      "66fbed4b87b94ab88416f77deebe73b6",
      "8e2a0dd52c0442a4bb170d411881a1b9",
      "c28ab20bd5fc4d96ad81c96654cba337",
      "3c4e20fa702c49beab70b1ba3df9d9e3",
      "50372e32e286456cafe67bc726c6a479"
     ]
    },
    "id": "ZlYr6wdSxelL",
    "outputId": "cf8ba539-1c53-471e-fed3-18d69a3dd0ba"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Load Base Model (4-bit Quantized)\n",
    "# =========================\n",
    "\"\"\"\n",
    "We load a pre-quantized 4-bit TinyLlama model provided by Unsloth.\n",
    "Benefits:\n",
    "- Very low VRAM usage\n",
    "- Fast download\n",
    "- Perfect for Colab / Kaggle / 8–16GB GPUs\n",
    "\"\"\"\n",
    "\n",
    "BASE_MODEL_NAME = \"unsloth/tinyllama-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,  # Enables long-context via RoPE scaling\n",
    "    dtype=dtype,                    # Auto FP16 / BF16\n",
    "    load_in_4bit=load_in_4bit,      # QLoRA-style memory efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM-pEdG3xgHH",
    "outputId": "2b03f880-be66-49ee-f83f-2a969d4d45d9"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Apply LoRA Adapters (PEFT)\n",
    "# =========================\n",
    "\"\"\"\n",
    "We attach LoRA adapters so that:\n",
    "- Only ~1–10% parameters are trained\n",
    "- Base model weights remain frozen\n",
    "- Training becomes faster and memory efficient\n",
    "\n",
    "Target modules:\n",
    "- q_proj, k_proj, v_proj, o_proj → Attention\n",
    "- gate_proj, up_proj, down_proj → MLP\n",
    "\"\"\"\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,                            # LoRA rank (8/16/32/64 are common)\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=32,                   # Usually = r or 2*r\n",
    "    lora_dropout=0.0,                # Unsloth recommends 0\n",
    "    bias=\"none\",                     # Best practice for LoRA\n",
    "    use_gradient_checkpointing=False,# Set True if model >= 7B\n",
    "    random_state=3407,               # Reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "b94da7f35654470d9f93741f55e0a522",
      "b0c91b69f63941e5a85b3061f760dec6",
      "e26eca0b7c5743d8ae45bcba2a8af36c",
      "73bc4349d663409b9e498afc2241285c",
      "0e70f847cc4f447a9348a546fce4d6ef",
      "cee3dae9ed80490ea26163e8434b23ea",
      "ccdeabb0a35b4b11b232080740e5a8e4",
      "1b55cecd0c844c179fab4d927c231e89",
      "bb9d01df3904479e9c75a41b2402d54a",
      "2a378905b2f448ae88591872eb4e4069",
      "bbfadb5aad444066ad464a332584ba1c",
      "b48a11eed6474aea92586d552adc3997",
      "688ac8b4c5514df1a09b95fa42606084",
      "5175b2992c044566965eca7bf858442b",
      "dc881abe60d841a581abd405a7ae270f",
      "510e25f9e20844af9831e4d30e428e97",
      "7e82da7401504c10a2a4e6ad3afa24f6",
      "329a48b128e846a9bf62fabc8c760f90",
      "993bc69cebf3469491d49dc45be84c32",
      "4f076ed3cb314889964e7913d3413325",
      "436b0a29497c4bd7989823dd5e2d73d2",
      "55e94d49c6354804bbd49c48133a1d69",
      "d5334c86542949159db226e289161fcd",
      "76c9b62d53434898a798d716a5fab898",
      "aa31ab41451742348d6d1ac4f2d917bb",
      "7d3920112ef3408a9845a0137d18e031",
      "29b654ee6a304d3bb90900347576ef10",
      "db71e855ab7544c6a77e34fd682b8fb4",
      "4374ecff828a46b3b490a3c3079d4c8b",
      "7d54dd7966d04a44ae3ac008c9d447de",
      "52696d129a6b42d3ad24004c8dfda62d",
      "c4e1ecf94aa14a33bb956fca91c22ea4",
      "537568e1091345c7a12c856e57d55b0b",
      "9152d8b3e86e4618b3095e465367188d",
      "41a7765a72d4436fb6f87a4dac8e7ebf",
      "d03a4fdb0ebc4748b3ba32847aad6bad",
      "088c4d70f2f1440d85792038e057b742",
      "573098915bca458386e06d7b50cce3f8",
      "8ee745f73f234a658bc15b0eb79969cc",
      "a2bb73d84eb24b52a44317c25e03b6bc",
      "010089c22697471988275f7f15bd3c16",
      "7b91f714b72a464c88b30f669ad2cad4",
      "fccf1ca42c564db299e3550266f0c99c",
      "545249bce3e24b7fad5be6b37f15c7ed"
     ]
    },
    "id": "saQ_uTvMxh9b",
    "outputId": "e6545c8a-3cb9-43f8-c7bc-b60f6fc4886b"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Dataset Preparation (Alpaca Format)\n",
    "# =========================\n",
    "\"\"\"\n",
    "We convert raw Alpaca data into a single 'text' field.\n",
    "IMPORTANT:\n",
    "- EOS token is mandatory, otherwise generation may never stop.\n",
    "\"\"\"\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_data(examples):\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(\n",
    "        examples[\"instruction\"],\n",
    "        examples[\"input\"],\n",
    "        examples[\"output\"],\n",
    "    ):\n",
    "        text = alpaca_prompt.format(\n",
    "            instruction,\n",
    "            input_text,\n",
    "            output\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Shuffle + small subset for fast demo training\n",
    "dataset = dataset.shuffle(seed=3407).select(range(2000))\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(\n",
    "    format_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,  # keep only 'text'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "86f4d41e67dd4c2f826a8a861fa1533f",
      "0e6f5e64d50d425da25bcb6d8a991862",
      "bf632818c03e4a1abefb322c6cdd71f1",
      "49fe40ab0dd346b2a4d536283f09674e",
      "458f313f850542b48cee241d5e3e2413",
      "b3f1bca912914d459ad38d65406555e2",
      "9e64a39f263f49a6ae12d1016ec2c30f",
      "e50d478d91b140d0ab16ebefb24403c7",
      "0acb12e9869244a2ab344745c03e5370",
      "fe72e9ae55814916bd3bf6aee2195abc",
      "9f5dd919616e436599bd36a16c357ae5"
     ]
    },
    "id": "bcL3bMvKxjC3",
    "outputId": "fc0c62a3-0430-43c7-d085-e55e976e4ce7"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Supervised Fine-Tuning (SFT)\n",
    "# =========================\n",
    "\"\"\"\n",
    "We use TRL's SFTTrainer with Unsloth optimizations.\n",
    "Key features:\n",
    "- Sequence packing → better GPU utilization\n",
    "- 8-bit optimizer → lower VRAM usage\n",
    "- Gradient accumulation → simulates larger batch size\n",
    "\"\"\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,  # Packs multiple short samples into one sequence\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,               # Stabilizes early training\n",
    "        optim=\"adamw_8bit\",             # Memory efficient optimizer\n",
    "        logging_steps=10,               # Progress visibility\n",
    "        seed=3407,                      # Reproducibility\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",               # Disable WandB by default\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg8PzBaIxkPb"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Inference (Fast Generation)\n",
    "# =========================\n",
    "\"\"\"\n",
    "Unsloth provides 2× faster inference using optimized kernels.\n",
    "IMPORTANT:\n",
    "- Always call FastLanguageModel.for_inference(model)\n",
    "- Disable gradients for inference\n",
    "\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"Continue the Fibonacci sequence\",\n",
    "    \"1, 1, 2, 3, 5, 8\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        use_cache=True,     # Faster decoding\n",
    "        do_sample=False     # Deterministic output for demo\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy1BzkOlxlX0"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. Save LoRA Adapters\n",
    "# =========================\n",
    "\"\"\"\n",
    "This saves ONLY the LoRA adapters, not the full base model.\n",
    "Benefits:\n",
    "- Very small size (~50–200 MB)\n",
    "- Can be merged later into FP16 / 4-bit / GGUF\n",
    "- Easy to share or version-control\n",
    "\"\"\"\n",
    "\n",
    "LORA_SAVE_PATH = \"lora_model\"\n",
    "\n",
    "model.save_pretrained(LORA_SAVE_PATH)\n",
    "tokenizer.save_pretrained(LORA_SAVE_PATH)\n",
    "\n",
    "print(f\"LoRA adapters saved at: {LORA_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3NELC5JXHbM"
   },
   "source": [
    "## unsloth and huggingface comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzZF_0sTXLoU"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) INSTALLS (COLAB)\n",
    "# =========================\n",
    "!pip -q install -U pip\n",
    "\n",
    "# Torch + CUDA is already on Colab typically. If needed, uncomment below:\n",
    "# !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Common deps\n",
    "!pip -q install -U datasets accelerate transformers trl peft bitsandbytes\n",
    "\n",
    "# Unsloth\n",
    "!pip -q install -U unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1s63O6OXQKF"
   },
   "outputs": [],
   "source": [
    "import time, torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU required\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "def reset_vram():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def peak_vram_gb():\n",
    "    return round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "\n",
    "def now():\n",
    "    return time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvJZaH8_MCgA"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def prepare_dataset(max_rows=200):\n",
    "    ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    ds = ds.select(range(max_rows))\n",
    "\n",
    "    def fmt(ex):\n",
    "        return {\n",
    "            \"text\": alpaca_prompt.format(\n",
    "                instruction=ex[\"instruction\"],\n",
    "                input=ex[\"input\"]\n",
    "            ) + ex[\"output\"]\n",
    "        }\n",
    "\n",
    "    return ds.map(fmt, remove_columns=ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4opZTft3XTvu"
   },
   "outputs": [],
   "source": [
    "def run_unsloth_benchmark(\n",
    "    model_name,\n",
    "    max_rows=200,\n",
    "    max_seq_length=1024,\n",
    "    steps=50,\n",
    "):\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    reset_vram()\n",
    "    start = now()\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    dataset = prepare_dataset(max_rows)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=SFTConfig(\n",
    "            max_steps=steps,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=10,\n",
    "            output_dir=\"unsloth_out\",\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    train_time = now() - start\n",
    "    train_vram = peak_vram_gb()\n",
    "\n",
    "    # Inference speed\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\"Explain LoRA in simple words.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = now()\n",
    "    out = model.generate(**inputs, max_new_tokens=128)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = now()\n",
    "\n",
    "    tokens_sec = out.shape[-1] / (t1 - t0)\n",
    "\n",
    "    return {\n",
    "        \"train_time_sec\": round(train_time, 2),\n",
    "        \"train_peak_vram_gb\": train_vram,\n",
    "        \"inference_tokens_per_sec\": round(tokens_sec, 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47eH9Q2QMG9B"
   },
   "outputs": [],
   "source": [
    "def run_hf_benchmark(\n",
    "    model_name,\n",
    "    max_rows=200,\n",
    "    max_seq_length=1024,\n",
    "    steps=50,\n",
    "):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    reset_vram()\n",
    "    start = now()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(\n",
    "        model,\n",
    "        LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\",\"v_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataset = prepare_dataset(max_rows)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=SFTConfig(\n",
    "            max_steps=steps,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=10,\n",
    "            output_dir=\"hf_out\",\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    train_time = now() - start\n",
    "    train_vram = peak_vram_gb()\n",
    "\n",
    "    inputs = tokenizer(\"Explain LoRA in simple words.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = now()\n",
    "    out = model.generate(**inputs, max_new_tokens=128)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = now()\n",
    "\n",
    "    tokens_sec = out.shape[-1] / (t1 - t0)\n",
    "\n",
    "    return {\n",
    "        \"train_time_sec\": round(train_time, 2),\n",
    "        \"train_peak_vram_gb\": train_vram,\n",
    "        \"inference_tokens_per_sec\": round(tokens_sec, 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-Kyu6uGMIq-"
   },
   "outputs": [],
   "source": [
    "unsloth_res = run_unsloth_benchmark(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    ")\n",
    "\n",
    "hf_res = run_hf_benchmark(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL COMPARISON\")\n",
    "for k in unsloth_res:\n",
    "    print(f\"{k}: Unsloth={unsloth_res[k]} | HF={hf_res[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6S3LitfiC-v"
   },
   "outputs": [],
   "source": [
    "When NOT to use Unsloth:\n",
    "- If you need heavy multi-node distributed training\n",
    "- If you want no-code UI only (LLaMA-Factory better)\n",
    "- If training classical ML models (not LLMs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

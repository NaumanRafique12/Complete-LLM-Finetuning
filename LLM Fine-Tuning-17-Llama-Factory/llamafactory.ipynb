{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBukQwDtiGPB",
    "outputId": "29407e94-9bfb-4059-cceb-4adc2b71baab"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey1wi-IuiNXK",
    "outputId": "c6c89f6c-e2cf-4578-a24b-35b7135dce25"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr0TVz8eifhW",
    "outputId": "8c5599e6-9e6b-4f76-ab5c-d6397bb9fb19"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoCCNUPgimnk",
    "outputId": "ba3e43ee-2d92-4f73-ff0c-d47bc0761200"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzTlLvieiNuv",
    "outputId": "37000c96-d6f2-4ad4-c8fd-43601feeb18e"
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNrLZhmEiXHx"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5jvvkUYjoYJ"
   },
   "outputs": [],
   "source": [
    "!ls /content/LLaMA-Factory/src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H709RivQnZUU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GRADIO_SHARE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75jh7-m1ig9u"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C62MBnsnkTgr"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes>=0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFoTL-suz1RT"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKs2c58CmDVs"
   },
   "outputs": [],
   "source": [
    "!python /content/LLaMA-Factory/src/webui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz0Wt5K2paVL"
   },
   "outputs": [],
   "source": [
    "from llamafactory.webui.interface import create_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYThCYQNpcS5"
   },
   "outputs": [],
   "source": [
    "ui = create_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmMbSP8gpYpT"
   },
   "outputs": [],
   "source": [
    "ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqL6T1QdxEOr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Base model (same as you used in training)\n",
    "base_model = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "# Path to your trained LoRA adapter\n",
    "lora_path = \"/content/LLaMA-Factory/saves/Gemma-1.1-2B-Instruct/lora/train_2025-12-04-18-53-45\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4hC_QgdxP0T"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA on top of base model\n",
    "model = PeftModel.from_pretrained(model, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0S9zXpTpkI4"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "\n",
    "# Enable evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# --- Inference prompt ---\n",
    "prompt = \"Write a short poem about the moon.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR40fbMfxwlX"
   },
   "source": [
    "##CLI Based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNQOeo0z887Z",
    "outputId": "80f57c5e-2f75-4da5-da43-f07d6d6989f9"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSM0XV6Dvrnm",
    "outputId": "93578796-0699-47c8-d374-b4c8bd1e90a5"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6JDyHYpxswp",
    "outputId": "3f3126db-4f94-4e50-8dab-4a8cedda32f0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd22mWDq8w79"
   },
   "outputs": [],
   "source": [
    "# python -m llamafactory.cli train \\\n",
    "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
    "#   --template gemma \\\n",
    "#   --stage sft \\\n",
    "#   --finetuning_type lora \\\n",
    "#   --dataset yahma/alpaca-cleaned \\\n",
    "#   --output_dir output/my-gemma-qlora \\\n",
    "#   --cutoff_len 2048 \\\n",
    "#   --per_device_train_batch_size 1 \\\n",
    "#   --gradient_accumulation_steps 8 \\\n",
    "#   --num_train_epochs 1 \\\n",
    "#   --learning_rate 5e-5 \\\n",
    "#   --lora_rank 64 \\\n",
    "#   --lora_alpha 16 \\\n",
    "#   --lora_dropout 0.05 \\\n",
    "#   --quantization_bit 4 \\\n",
    "#   --fp16 True \\\n",
    "#   --gradient_checkpointing True \\\n",
    "#   --save_strategy epoch \\\n",
    "#   --save_steps 0 \\\n",
    "#   --save_total_limit 3 \\\n",
    "#   --logging_steps 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYJ325HdH5Lx",
    "outputId": "2f4cfeca-0636-4302-d8c1-66abf916ec5a"
   },
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66W9fQA0uIWM",
    "outputId": "a6a4a4bd-091e-473e-f29d-82dc12a92066"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli train train_gemma_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OzqKWLcC7ol"
   },
   "outputs": [],
   "source": [
    "!ls output/my-gemma-qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OxnKVQhAM4V",
    "outputId": "fe1170fb-d992-4eeb-bd86-6f491687a2e0"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli train test.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KvNaOPnBbR2",
    "outputId": "adc6f80c-6fa4-4052-d3c5-9c52a05b96f0"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli train \\\n",
    "  --model_name_or_path google/gemma-1.1-2b-it \\\n",
    "  --template gemma \\\n",
    "  --finetuning_type lora \\\n",
    "  --dataset yahma/alpaca-cleaned \\\n",
    "  --output_dir output/debug \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --lora_rank 32 --lora_alpha 16 --lora_dropout 0.05 \\\n",
    "  --save_strategy epoch --save_total_limit 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cUT-URABqDK",
    "outputId": "1cf7704f-7f59-4423-dbb1-674f6fc8c61e"
   },
   "outputs": [],
   "source": [
    "!df -h\n",
    "!touch output/debug/testfile && ls output/debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUkmHvREASDZ"
   },
   "outputs": [],
   "source": [
    "!ls output/test-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RHYpdS2BESd",
    "outputId": "69dc4853-80b0-4dc3-be78-6d303687db70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"output/test-lora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458,
     "referenced_widgets": [
      "2a14756366f048c990c569f4e96794f3",
      "8f4ac9dde7344396b76036d964da31b9",
      "7386b31891024d6596ff5f6a32fa4cd0",
      "17bdcef1f0334a4681647fbbad3f3ef3",
      "e9f054691e9545e0ba627c7b33e33c98",
      "3fe013bf54824818b01676cb5842642e",
      "518d22c6903c42988786eefe77a8774a",
      "de57a09e83c34ab2baa9abfaeea6fe71",
      "296b464e196c4b2da2acd69501410f79",
      "4ec2cbe29f6343dd927c257b909524a6",
      "171e9377a40b4663a2f85c9215dd33a5"
     ]
    },
    "id": "H46HTzcvuVTQ",
    "outputId": "33bcc7e9-bf78-4208-ff37-032c88d9b1d5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base = \"google/gemma-1.1-2b-it\"\n",
    "adapter = \"/content/LLaMA-Factory/output/llama3-qlora\"   # your LoRA folder path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "\n",
    "prompt = \"Explain what is QLoRA\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGRj2u2a_Uxw"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIWGecfpuXWE"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli export train_gemma_qlora.yaml\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBukQwDtiGPB",
        "outputId": "0e10f51c-31c5-4739-bb8e-204384050b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 24769, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 24769 (delta 45), reused 19 (delta 19), pack-reused 24686 (from 3)\u001b[K\n",
            "Receiving objects: 100% (24769/24769), 12.28 MiB | 17.20 MiB/s, done.\n",
            "Resolving deltas: 100% (17810/17810), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPBNia3W8A1P",
        "outputId": "3f403f46-ce46-4971-ee9e-7e89aae25f0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey1wi-IuiNXK",
        "outputId": "83c6e90b-89a1-48cf-f7fd-b03f96de47b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr0TVz8eifhW",
        "outputId": "a3f89ddd-e033-4cf6-b17c-f420b7828e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoCCNUPgimnk",
        "outputId": "3f3118a7-9ca8-4719-92b4-365849098638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t      docker\tMakefile\tREADME.md\t  scripts   tests\n",
            "CITATION.cff  examples\tMANIFEST.in\tREADME_zh.md\t  setup.py  tests_v1\n",
            "data\t      LICENSE\tpyproject.toml\trequirements.txt  src\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kg8J4red8S1j",
        "outputId": "bcd95d78-b82b-4d58-a22e-edbef968d886"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring transformers: markers 'python_version < \"3.10\"' don't match your environment\n",
            "Collecting transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0 (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.0.0)\n",
            "Collecting accelerate<=1.11.0,>=1.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.17.1,>=0.14.0 (from -r requirements.txt (line 6))\n",
            "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 7))\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio<=5.45.0,>=4.38.0 (from -r requirements.txt (line 9))\n",
            "  Downloading gradio-5.45.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
            "Collecting tyro<0.9.0 (from -r requirements.txt (line 11))\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.8.1)\n",
            "Collecting numpy<2.0.0 (from -r requirements.txt (line 14))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (0.12.0)\n",
            "Collecting modelscope>=1.14.0 (from -r requirements.txt (line 20))\n",
            "  Downloading modelscope-1.33.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hf-transfer in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (0.1.9)\n",
            "Collecting safetensors<=0.5.3 (from -r requirements.txt (line 22))\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting fire (from -r requirements.txt (line 24))\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (6.0.3)\n",
            "Collecting pydantic<=2.10.6 (from -r requirements.txt (line 29))\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (0.38.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (0.118.3)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (3.0.3)\n",
            "Collecting av (from -r requirements.txt (line 35))\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (0.11.0)\n",
            "Requirement already satisfied: propcache!=0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 38)) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (2.9.0+cu126)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (1.0.0)\n",
            "Collecting gradio-client==1.13.0 (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9))\n",
            "  Downloading gradio_client-1.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (3.11.4)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 10)) (2.9.0.post0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->-r requirements.txt (line 11)) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->-r requirements.txt (line 11)) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 11))\n",
            "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 15)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 15)) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->-r requirements.txt (line 20)) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->-r requirements.txt (line 20)) (2.5.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->-r requirements.txt (line 24)) (3.2.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->-r requirements.txt (line 25)) (4.9.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->-r requirements.txt (line 29)) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->-r requirements.txt (line 29))\n",
            "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->-r requirements.txt (line 31)) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->-r requirements.txt (line 31)) (0.16.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 36)) (1.1.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 36)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 36)) (4.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 10)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->-r requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 11)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 36)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 36)) (2.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->-r requirements.txt (line 9)) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->-r requirements.txt (line 4)) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 36)) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.45.0-py3-none-any.whl (60.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.13.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.33.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, safetensors, pydantic-core, numpy, fire, av, pydantic, modelscope, tyro, gradio-client, transformers, gradio, accelerate, trl, peft\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.7.0\n",
            "    Uninstalling safetensors-0.7.0:\n",
            "      Successfully uninstalled safetensors-0.7.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.4\n",
            "    Uninstalling pydantic_core-2.41.4:\n",
            "      Successfully uninstalled pydantic_core-2.41.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.3\n",
            "    Uninstalling pydantic-2.12.3:\n",
            "      Successfully uninstalled pydantic-2.12.3\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.23.1 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.11.0 av-16.0.1 fire-0.7.1 gradio-5.45.0 gradio-client-1.13.0 modelscope-1.33.0 numpy-1.26.4 peft-0.17.1 pydantic-2.10.6 pydantic-core-2.27.2 safetensors-0.5.3 shtab-1.8.0 transformers-4.57.1 trl-0.9.6 tyro-0.8.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e1bfd427734b4fc29edc423bbaffe2bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes>=0.39.0"
      ],
      "metadata": {
        "id": "Jv1dXn6q8dzj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMaSCbvOQUFO",
        "outputId": "5b3efa76-7874-41e2-b08e-ee663da21301"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMwjgyWN8wiW",
        "outputId": "17e5b652-f8eb-4774-f974-f08d7529d412"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaeTAn1086Oi",
        "outputId": "4a8a543d-e3e5-4bb8-c375-2219ad238956"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t      docker\tMakefile\tREADME.md\t  scripts   tests\n",
            "CITATION.cff  examples\tMANIFEST.in\tREADME_zh.md\t  setup.py  tests_v1\n",
            "data\t      LICENSE\tpyproject.toml\trequirements.txt  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzTlLvieiNuv",
        "outputId": "9ebcdb5c-9e55-4ef0-8d2d-28825182a6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (4.57.1)\n",
            "Requirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (4.0.0)\n",
            "Requirement already satisfied: accelerate<=1.11.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.11.0)\n",
            "Requirement already satisfied: peft<=0.17.1,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.17.1)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio<=5.45.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (5.45.0)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.10.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.8.14)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.12.0)\n",
            "Requirement already satisfied: modelscope>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.33.0)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
            "Requirement already satisfied: safetensors<=0.5.3 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.5.3)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.7.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (6.0.3)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.10.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.38.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.118.3)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (16.0.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: propcache!=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2025.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11.4)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (2.27.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (0.22.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=28947 sha256=1ef2aa156ee7ae0bffe2ce2643908cbdc62583b4ff94c8d3d9b3e9fd4fe37b8d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nd_h6xeu/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "Successfully installed llamafactory-0.9.4.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5jvvkUYjoYJ",
        "outputId": "0c22d53a-9706-4847-da35-39aa713fd9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "api.py\tllamafactory  llamafactory.egg-info  train.py  webui.py\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H709RivQnZUU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GRADIO_SHARE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75jh7-m1ig9u",
        "outputId": "80bfb7ea-64d1-4337-999b-ae48a8d33469"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "The token `testinglatest` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `testinglatest`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate transformers peft"
      ],
      "metadata": {
        "id": "eFoTL-suz1RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKs2c58CmDVs",
        "outputId": "c8d4062d-bbe7-4e65-9030-489c0b6fb90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-11 17:40:14.707179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765474814.732814   21236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765474814.741134   21236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765474814.777252   21236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765474814.777278   21236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765474814.777282   21236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765474814.777286   21236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-11 17:40:14.783583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://c15d428f2ca29f8621.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "[WARNING|2025-12-11 17:49:42] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2025-12-11 17:49:42] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|2025-12-11 17:49:45] llamafactory.data.template:143 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2025-12-11 17:49:45] llamafactory.data.loader:143 >> Loading dataset harpomaxx/unix-commands...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 2045, 708, 1490, 476, 102599, 14837, 17145, 235265, 1646, 2027, 578, 8702, 7880, 685, 476, 102599, 17145, 235265, 108, 4851, 235348, 20156, 27744, 6274, 235345, 80836, 110, 107, 108, 106, 2516, 108, 235283, 6274, 235283, 4851, 235248, 108, 4851, 235348, 20156, 27744, 6274, 235345, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "You are now a Unix OS terminal. You act and respond exactly as a Unix terminal.\n",
            "root@localhost:/home# pwd\n",
            "\n",
            "\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "/home/root \n",
            "root@localhost:/home#<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235283, 6274, 235283, 4851, 235248, 108, 4851, 235348, 20156, 27744, 6274, 235345, 107, 108]\n",
            "labels:\n",
            "/home/root \n",
            "root@localhost:/home#<end_of_turn>\n",
            "\n",
            "[INFO|2025-12-11 17:49:48] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-12-11 17:49:48] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|2025-12-11 17:50:16] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-12-11 17:50:16] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-11 17:50:16] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-12-11 17:50:16] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-12-11 17:50:16] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,q_proj,down_proj,k_proj,v_proj,gate_proj,up_proj\n",
            "[INFO|2025-12-11 17:50:17] llamafactory.model.loader:143 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|2025-12-11 17:50:36] llamafactory.train.callbacks:143 >> {'loss': 2.7500, 'learning_rate': 1.9437e-05, 'epoch': 0.80, 'throughput': 673.15}\n",
            "{'loss': 2.75, 'grad_norm': 3.492421865463257, 'learning_rate': 1.9436976651092144e-05, 'epoch': 0.8, 'num_input_tokens_seen': 12416, 'train_runtime': 18.4473, 'train_tokens_per_second': 673.052}\n",
            "{'train_runtime': 24.4702, 'train_samples_per_second': 4.087, 'train_steps_per_second': 0.286, 'train_loss': 2.7155506951468333, 'epoch': 1.0, 'num_input_tokens_seen': 15744}\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  num_input_tokens_seen    =      15744\n",
            "  total_flos               =   175221GF\n",
            "  train_loss               =     2.7156\n",
            "  train_runtime            = 0:00:24.47\n",
            "  train_samples_per_second =      4.087\n",
            "  train_steps_per_second   =      0.286\n",
            "Figure saved at: saves/Gemma-1.1-2B-Instruct/lora/train_2025-12-11-17-40-51/training_loss.png\n",
            "[WARNING|2025-12-11 17:50:42] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-12-11 17:50:42] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:02,431 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:765] 2025-12-11 17:53:04,024 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-11 17:53:04,026 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 17:53:04,206 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-12-11 17:53:05] llamafactory.data.template:143 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|configuration_utils.py:765] 2025-12-11 17:53:05,353 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-11 17:53:05,354 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-12-11 17:53:05,354 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|2025-12-11 17:53:05] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-12-11 17:53:05] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[WARNING|logging.py:328] 2025-12-11 17:53:05,871 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1172] 2025-12-11 17:53:09,171 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2341] 2025-12-11 17:53:09,172 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:986] 2025-12-11 17:53:09,173 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:23<00:00, 11.52s/it]\n",
            "[INFO|configuration_utils.py:941] 2025-12-11 17:53:32,427 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:986] 2025-12-11 17:53:32,427 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:423] 2025-12-11 17:53:32,520 >> Could not locate the custom_generate/generate.py inside google/gemma-1.1-2b-it.\n",
            "[INFO|2025-12-11 17:53:32] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-11 17:53:32] llamafactory.model.loader:143 >> all params: 2,506,172,416\n",
            "[WARNING|2025-12-11 17:53:32] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n",
            "Keyboard interruption in main thread... closing server.Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3022, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/webui.py\", line 31, in <module>\n",
            "    main()\n",
            "  File \"/content/LLaMA-Factory/src/webui.py\", line 27, in main\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2919, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3024, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/console_capture.py\", line 177, in write_with_callbacks\n",
            "    stack.enter_context(wb_logging.log_to_all_runs())\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 301, in helper\n",
            "    return _GeneratorContextManager(func, args, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 108, in __init__\n",
            "    doc = getattr(func, \"__doc__\", None)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://c15d428f2ca29f8621.gradio.live\n"
          ]
        }
      ],
      "source": [
        "!python /content/LLaMA-Factory/src/webui.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz0Wt5K2paVL"
      },
      "outputs": [],
      "source": [
        "from llamafactory.webui.interface import create_ui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYThCYQNpcS5"
      },
      "outputs": [],
      "source": [
        "ui = create_ui()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmMbSP8gpYpT"
      },
      "outputs": [],
      "source": [
        "ui.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Base model (same as you used in training)\n",
        "base_model = \"google/gemma-1.1-2b-it\"\n",
        "\n",
        "# Path to your trained LoRA adapter\n",
        "lora_path = \"/content/LLaMA-Factory/saves/Gemma-1.1-2B-Instruct/lora/train_2025-12-11-17-40-51\""
      ],
      "metadata": {
        "id": "uqL6T1QdxEOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Apply LoRA on top of base model\n",
        "model = PeftModel.from_pretrained(model, lora_path)"
      ],
      "metadata": {
        "id": "x4hC_QgdxP0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0S9zXpTpkI4"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "\n",
        "\n",
        "# Enable evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# --- Inference prompt ---\n",
        "prompt = \"Can you tell what ls -l would display?\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLI Based methods"
      ],
      "metadata": {
        "id": "vR40fbMfxwlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m llamafactory.cli train \\\n",
        "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
        "#   --template gemma \\\n",
        "#   --stage sft \\\n",
        "#   --finetuning_type lora \\\n",
        "#   --dataset yahma/alpaca-cleaned \\\n",
        "#   --output_dir output/my-gemma-qlora \\\n",
        "#   --cutoff_len 2048 \\\n",
        "#   --per_device_train_batch_size 1 \\\n",
        "#   --gradient_accumulation_steps 8 \\\n",
        "#   --num_train_epochs 1 \\\n",
        "#   --learning_rate 5e-5 \\\n",
        "#   --lora_rank 64 \\\n",
        "#   --lora_alpha 16 \\\n",
        "#   --lora_dropout 0.05 \\\n",
        "#   --quantization_bit 4 \\\n",
        "#   --fp16 True \\\n",
        "#   --gradient_checkpointing True \\\n",
        "#   --save_strategy epoch \\\n",
        "#   --save_steps 0 \\\n",
        "#   --save_total_limit 3 \\\n",
        "#   --logging_steps 10"
      ],
      "metadata": {
        "id": "y9ER9Xn0kAj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNQOeo0z887Z",
        "outputId": "ff5bf146-de4f-43ca-da41-a485075ef1f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSM0XV6Dvrnm",
        "outputId": "5a1e6bf2-5301-4e8f-d446-117c32f959ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6JDyHYpxswp",
        "outputId": "2437d5c0-b9c1-40b7-a7d6-3d4062277159"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 11 18:03:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYJ325HdH5Lx",
        "outputId": "fa450313-bf82-4f72-fe03-ca013fcc0484"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llamafactory.cli train train_gemma_qlora.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66W9fQA0uIWM",
        "outputId": "e5c7ecc4-c231-4bfd-cd76-358d859cb773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-11 18:04:21.876545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765476261.896441   27309 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765476261.902681   27309 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765476261.918097   27309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765476261.918128   27309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765476261.918134   27309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765476261.918137   27309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-11 18:04:21.922697: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[WARNING|2025-12-11 18:04:30] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2025-12-11 18:04:30] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:30,920 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:765] 2025-12-11 18:04:32,604 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-11 18:04:32,608 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,796 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,797 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,797 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,797 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,797 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-11 18:04:32,797 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-12-11 18:04:33] llamafactory.data.template:143 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2025-12-11 18:04:33] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...\n",
            "Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 999 examples [00:00, 13967.36 examples/s]\n",
            "Converting format of dataset (num_proc=4): 100% 999/999 [00:00<00:00, 2463.37 examples/s]\n",
            "Running tokenizer on dataset (num_proc=4): 100% 999/999 [00:08<00:00, 112.14 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 50721, 476, 2185, 576, 3547, 223482, 235265, 107, 108, 106, 2516, 108, 31119, 223482, 603, 671, 3980, 578, 15855, 2185, 235341, 5698, 708, 4065, 235290, 1547, 235290, 8847, 12027, 611, 1368, 577, 1501, 1174, 235292, 109, 235274, 235265, 96352, 861, 15555, 235265, 1699, 6859, 223482, 235269, 692, 235303, 529, 1476, 235292, 235248, 235274, 7190, 832, 235290, 38197, 16251, 235269, 235248, 235284, 12776, 235269, 235248, 235274, 235283, 235284, 7190, 9512, 235269, 235248, 235274, 235283, 235284, 7190, 2003, 235269, 235248, 235274, 235283, 235310, 30882, 9193, 235269, 578, 235248, 235284, 43212, 47362, 10605, 235265, 109, 235284, 235265, 19392, 573, 7450, 235292, 878, 476, 2910, 25373, 14581, 235269, 80196, 3584, 573, 16251, 578, 573, 12776, 235265, 142473, 1843, 573, 9512, 578, 2003, 235269, 52174, 16263, 577, 7433, 674, 1104, 708, 793, 127675, 235265, 4463, 9193, 578, 47362, 10605, 235269, 578, 7345, 1578, 235265, 109, 235304, 235265, 4371, 573, 7450, 2066, 235292, 1927, 692, 798, 235269, 2142, 573, 7450, 5045, 604, 671, 6370, 689, 712, 235265, 1417, 877, 1707, 573, 16251, 577, 33398, 573, 10177, 578, 1501, 573, 223482, 978, 17580, 235265, 109, 235310, 235265, 21046, 861, 2959, 235292, 127940, 476, 2173, 235290, 33107, 2959, 1163, 8890, 6719, 235265, 226437, 10605, 573, 2959, 689, 1281, 15983, 13770, 577, 7704, 573, 223482, 774, 49004, 235265, 109, 235308, 235265, 12456, 573, 7450, 235292, 12266, 476, 221363, 689, 476, 21385, 7190, 235269, 1982, 476, 2301, 3619, 576, 7450, 591, 11082, 235248, 235274, 235283, 235310, 7190, 235275, 10401, 573, 5086, 576, 573, 2959, 235265, 68122, 52777, 573, 2959, 575, 476, 20856, 8252, 577, 8151, 573, 7450, 48145, 578, 108585, 1163, 573, 6837, 576, 573, 2959, 235265, 109, 235318, 235265, 7110, 573, 112137, 235292, 7110, 573, 112137, 604, 235248, 235274, 235290, 235284, 4363, 3274, 573, 6837, 603, 34620, 13658, 235265, 127401, 98285, 573, 17370, 675, 476, 128765, 578, 24740, 573, 112137, 1163, 577, 3425, 573, 1156, 2857, 604, 2550, 9428, 235265, 109, 235324, 235265, 17691, 578, 17399, 235292, 138056, 9499, 573, 112137, 10401, 476, 8811, 235269, 578, 1492, 17399, 573, 2185, 675, 573, 11548, 7450, 235265, 24250, 577, 582, 235290, 54541, 573, 2959, 1865, 1853, 112137, 1013, 4647, 235265, 109, 235321, 235265, 31831, 578, 8106, 235292, 31831, 861, 26405, 223482, 675, 861, 13142, 20585, 235269, 1582, 685, 6422, 9471, 235269, 72504, 9450, 235269, 202770, 235269, 689, 12578, 578, 13624, 235265, 22812, 689, 13528, 235269, 578, 8106, 7544, 235265, 23646, 235341, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Describe a process of making crepes.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 31119, 223482, 603, 671, 3980, 578, 15855, 2185, 235341, 5698, 708, 4065, 235290, 1547, 235290, 8847, 12027, 611, 1368, 577, 1501, 1174, 235292, 109, 235274, 235265, 96352, 861, 15555, 235265, 1699, 6859, 223482, 235269, 692, 235303, 529, 1476, 235292, 235248, 235274, 7190, 832, 235290, 38197, 16251, 235269, 235248, 235284, 12776, 235269, 235248, 235274, 235283, 235284, 7190, 9512, 235269, 235248, 235274, 235283, 235284, 7190, 2003, 235269, 235248, 235274, 235283, 235310, 30882, 9193, 235269, 578, 235248, 235284, 43212, 47362, 10605, 235265, 109, 235284, 235265, 19392, 573, 7450, 235292, 878, 476, 2910, 25373, 14581, 235269, 80196, 3584, 573, 16251, 578, 573, 12776, 235265, 142473, 1843, 573, 9512, 578, 2003, 235269, 52174, 16263, 577, 7433, 674, 1104, 708, 793, 127675, 235265, 4463, 9193, 578, 47362, 10605, 235269, 578, 7345, 1578, 235265, 109, 235304, 235265, 4371, 573, 7450, 2066, 235292, 1927, 692, 798, 235269, 2142, 573, 7450, 5045, 604, 671, 6370, 689, 712, 235265, 1417, 877, 1707, 573, 16251, 577, 33398, 573, 10177, 578, 1501, 573, 223482, 978, 17580, 235265, 109, 235310, 235265, 21046, 861, 2959, 235292, 127940, 476, 2173, 235290, 33107, 2959, 1163, 8890, 6719, 235265, 226437, 10605, 573, 2959, 689, 1281, 15983, 13770, 577, 7704, 573, 223482, 774, 49004, 235265, 109, 235308, 235265, 12456, 573, 7450, 235292, 12266, 476, 221363, 689, 476, 21385, 7190, 235269, 1982, 476, 2301, 3619, 576, 7450, 591, 11082, 235248, 235274, 235283, 235310, 7190, 235275, 10401, 573, 5086, 576, 573, 2959, 235265, 68122, 52777, 573, 2959, 575, 476, 20856, 8252, 577, 8151, 573, 7450, 48145, 578, 108585, 1163, 573, 6837, 576, 573, 2959, 235265, 109, 235318, 235265, 7110, 573, 112137, 235292, 7110, 573, 112137, 604, 235248, 235274, 235290, 235284, 4363, 3274, 573, 6837, 603, 34620, 13658, 235265, 127401, 98285, 573, 17370, 675, 476, 128765, 578, 24740, 573, 112137, 1163, 577, 3425, 573, 1156, 2857, 604, 2550, 9428, 235265, 109, 235324, 235265, 17691, 578, 17399, 235292, 138056, 9499, 573, 112137, 10401, 476, 8811, 235269, 578, 1492, 17399, 573, 2185, 675, 573, 11548, 7450, 235265, 24250, 577, 582, 235290, 54541, 573, 2959, 1865, 1853, 112137, 1013, 4647, 235265, 109, 235321, 235265, 31831, 578, 8106, 235292, 31831, 861, 26405, 223482, 675, 861, 13142, 20585, 235269, 1582, 685, 6422, 9471, 235269, 72504, 9450, 235269, 202770, 235269, 689, 12578, 578, 13624, 235265, 22812, 689, 13528, 235269, 578, 8106, 7544, 235265, 23646, 235341, 107, 108]\n",
            "labels:\n",
            "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:765] 2025-12-11 18:04:44,087 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-11 18:04:44,089 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2025-12-11 18:04:44] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-12-11 18:04:44] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[WARNING|logging.py:328] 2025-12-11 18:04:44,959 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1172] 2025-12-11 18:04:48,741 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2341] 2025-12-11 18:04:48,742 >> Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:986] 2025-12-11 18:04:48,744 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:10<00:00,  5.47s/it]\n",
            "[INFO|configuration_utils.py:941] 2025-12-11 18:05:00,279 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:986] 2025-12-11 18:05:00,279 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:423] 2025-12-11 18:05:00,381 >> Could not locate the custom_generate/generate.py inside google/gemma-1.1-2b-it.\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,q_proj,up_proj,gate_proj,k_proj,v_proj,o_proj\n",
            "[INFO|2025-12-11 18:05:00] llamafactory.model.loader:143 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|trainer.py:749] 2025-12-11 18:05:00,865 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:982] 2025-12-11 18:05:00,867 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 107}.\n",
            "[INFO|trainer.py:2519] 2025-12-11 18:05:01,331 >> ***** Running training *****\n",
            "[INFO|trainer.py:2520] 2025-12-11 18:05:01,331 >>   Num examples = 899\n",
            "[INFO|trainer.py:2521] 2025-12-11 18:05:01,331 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2522] 2025-12-11 18:05:01,331 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2525] 2025-12-11 18:05:01,331 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2526] 2025-12-11 18:05:01,331 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2527] 2025-12-11 18:05:01,331 >>   Total optimization steps = 225\n",
            "[INFO|trainer.py:2528] 2025-12-11 18:05:01,334 >>   Number of trainable parameters = 9,805,824\n",
            "[INFO|integration_utils.py:867] 2025-12-11 18:05:01,341 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/offline-run-20251211_180551-vamasj1d\u001b[0m\n",
            "{'loss': 2.516, 'grad_norm': 5.960846424102783, 'learning_rate': 9.997923381619256e-05, 'epoch': 0.04}\n",
            "{'loss': 1.7471, 'grad_norm': 2.080181360244751, 'learning_rate': 9.925422696325975e-05, 'epoch': 0.09}\n",
            "{'loss': 1.52, 'grad_norm': 1.858041524887085, 'learning_rate': 9.750809600145954e-05, 'epoch': 0.13}\n",
            "{'loss': 1.4357, 'grad_norm': 3.7652251720428467, 'learning_rate': 9.477704120297697e-05, 'epoch': 0.18}\n",
            "{'loss': 1.4781, 'grad_norm': 2.3318731784820557, 'learning_rate': 9.111768199053588e-05, 'epoch': 0.22}\n",
            "{'loss': 1.3002, 'grad_norm': 1.9180926084518433, 'learning_rate': 8.660588312022344e-05, 'epoch': 0.27}\n",
            "{'loss': 1.3902, 'grad_norm': 1.6915203332901, 'learning_rate': 8.133518187573862e-05, 'epoch': 0.31}\n",
            "{'loss': 1.4084, 'grad_norm': 1.405611276626587, 'learning_rate': 7.541484888100974e-05, 'epoch': 0.36}\n",
            " 38% 86/225 [03:34<05:43,  2.47s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls output/my-gemma-qlora"
      ],
      "metadata": {
        "id": "2OzqKWLcC7ol"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m llamafactory.cli train test.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OxnKVQhAM4V",
        "outputId": "fe1170fb-d992-4eeb-bd86-6f491687a2e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 07:27:16.172992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765265236.192342   24859 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765265236.198212   24859 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765265236.212919   24859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265236.212943   24859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265236.212947   24859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265236.212951   24859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO|2025-12-09 07:27:25] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,440 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,441 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,441 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,441 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,441 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:25,441 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 07:27:27,147 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 07:27:27,150 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,334 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,335 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,335 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,335 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,335 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:27:27,335 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-12-09 07:27:28] llamafactory.data.template:143 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2025-12-09 07:27:28] llamafactory.data.loader:143 >> Loading dataset yahma/alpaca-cleaned...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 23136, 2149, 9697, 604, 23213, 9606, 235265, 107, 108, 106, 2516, 108, 235274, 235265, 38141, 476, 24748, 578, 97762, 11652, 235292, 8394, 2821, 861, 21305, 708, 28540, 576, 476, 8080, 576, 16803, 578, 19574, 235269, 19163, 9646, 235269, 3733, 29907, 235269, 578, 9606, 61926, 235265, 1417, 7154, 577, 3658, 861, 2971, 675, 573, 8727, 37132, 577, 1411, 696, 1277, 1963, 578, 798, 1707, 7704, 19424, 16011, 235265, 109, 235284, 235265, 100922, 575, 5748, 6915, 5640, 235292, 30896, 603, 20305, 604, 22522, 3779, 22241, 235269, 22488, 235269, 578, 52138, 2962, 235265, 67087, 604, 696, 3476, 235248, 235274, 235308, 235276, 4363, 576, 23135, 104941, 9987, 689, 235248, 235324, 235308, 4363, 576, 48679, 9987, 1853, 2788, 235265, 109, 235304, 235265, 3512, 3594, 6284, 235292, 31458, 3594, 3614, 6284, 603, 20305, 604, 6915, 578, 9663, 1578, 235290, 16978, 235265, 1165, 7154, 577, 41748, 18068, 235269, 4771, 31572, 1411, 235269, 578, 15093, 9606, 5115, 578, 24091, 1411, 235265, 67087, 604, 235248, 235324, 235290, 235315, 3763, 576, 6284, 1853, 3354, 235265, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Give three tips for staying healthy.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235274, 235265, 38141, 476, 24748, 578, 97762, 11652, 235292, 8394, 2821, 861, 21305, 708, 28540, 576, 476, 8080, 576, 16803, 578, 19574, 235269, 19163, 9646, 235269, 3733, 29907, 235269, 578, 9606, 61926, 235265, 1417, 7154, 577, 3658, 861, 2971, 675, 573, 8727, 37132, 577, 1411, 696, 1277, 1963, 578, 798, 1707, 7704, 19424, 16011, 235265, 109, 235284, 235265, 100922, 575, 5748, 6915, 5640, 235292, 30896, 603, 20305, 604, 22522, 3779, 22241, 235269, 22488, 235269, 578, 52138, 2962, 235265, 67087, 604, 696, 3476, 235248, 235274, 235308, 235276, 4363, 576, 23135, 104941, 9987, 689, 235248, 235324, 235308, 4363, 576, 48679, 9987, 1853, 2788, 235265, 109, 235304, 235265, 3512, 3594, 6284, 235292, 31458, 3594, 3614, 6284, 603, 20305, 604, 6915, 578, 9663, 1578, 235290, 16978, 235265, 1165, 7154, 577, 41748, 18068, 235269, 4771, 31572, 1411, 235269, 578, 15093, 9606, 5115, 578, 24091, 1411, 235265, 67087, 604, 235248, 235324, 235290, 235315, 3763, 576, 6284, 1853, 3354, 235265, 107, 108]\n",
            "labels:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 07:27:31,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 07:27:31,143 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-12-09 07:27:31,143 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|2025-12-09 07:27:31] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[WARNING|logging.py:328] 2025-12-09 07:27:32,045 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1172] 2025-12-09 07:27:32,045 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2341] 2025-12-09 07:27:32,047 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:986] 2025-12-09 07:27:32,048 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:22<00:00, 11.39s/it]\n",
            "[INFO|configuration_utils.py:941] 2025-12-09 07:27:54,954 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:986] 2025-12-09 07:27:54,954 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:423] 2025-12-09 07:27:55,044 >> Could not locate the custom_generate/generate.py inside google/gemma-1.1-2b-it.\n",
            "[INFO|2025-12-09 07:27:55] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-09 07:27:55] llamafactory.model.loader:143 >> all params: 2,506,172,416\n",
            "[WARNING|trainer.py:906] 2025-12-09 07:27:55,095 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m llamafactory.cli train \\\n",
        "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
        "#   --template gemma \\\n",
        "#   --finetuning_type lora \\\n",
        "#   --dataset yahma/alpaca-cleaned \\\n",
        "#   --output_dir output/debug \\\n",
        "#   --per_device_train_batch_size 1 \\\n",
        "#   --gradient_accumulation_steps 8 \\\n",
        "#   --num_train_epochs 1 \\\n",
        "#   --lora_rank 32 --lora_alpha 16 --lora_dropout 0.05 \\\n",
        "#   --save_strategy epoch --save_total_limit 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KvNaOPnBbR2",
        "outputId": "adc6f80c-6fa4-4052-d3c5-9c52a05b96f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 07:31:40.593814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765265500.616322   25999 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765265500.622594   25999 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765265500.638076   25999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265500.638102   25999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265500.638106   25999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765265500.638110   25999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO|2025-12-09 07:31:49] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:49,865 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 07:31:52,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 07:31:52,008 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,188 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,188 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,188 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,188 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,188 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 07:31:52,189 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-12-09 07:31:53] llamafactory.data.template:143 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2025-12-09 07:31:53] llamafactory.data.loader:143 >> Loading dataset yahma/alpaca-cleaned...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 23136, 2149, 9697, 604, 23213, 9606, 235265, 107, 108, 106, 2516, 108, 235274, 235265, 38141, 476, 24748, 578, 97762, 11652, 235292, 8394, 2821, 861, 21305, 708, 28540, 576, 476, 8080, 576, 16803, 578, 19574, 235269, 19163, 9646, 235269, 3733, 29907, 235269, 578, 9606, 61926, 235265, 1417, 7154, 577, 3658, 861, 2971, 675, 573, 8727, 37132, 577, 1411, 696, 1277, 1963, 578, 798, 1707, 7704, 19424, 16011, 235265, 109, 235284, 235265, 100922, 575, 5748, 6915, 5640, 235292, 30896, 603, 20305, 604, 22522, 3779, 22241, 235269, 22488, 235269, 578, 52138, 2962, 235265, 67087, 604, 696, 3476, 235248, 235274, 235308, 235276, 4363, 576, 23135, 104941, 9987, 689, 235248, 235324, 235308, 4363, 576, 48679, 9987, 1853, 2788, 235265, 109, 235304, 235265, 3512, 3594, 6284, 235292, 31458, 3594, 3614, 6284, 603, 20305, 604, 6915, 578, 9663, 1578, 235290, 16978, 235265, 1165, 7154, 577, 41748, 18068, 235269, 4771, 31572, 1411, 235269, 578, 15093, 9606, 5115, 578, 24091, 1411, 235265, 67087, 604, 235248, 235324, 235290, 235315, 3763, 576, 6284, 1853, 3354, 235265, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Give three tips for staying healthy.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235274, 235265, 38141, 476, 24748, 578, 97762, 11652, 235292, 8394, 2821, 861, 21305, 708, 28540, 576, 476, 8080, 576, 16803, 578, 19574, 235269, 19163, 9646, 235269, 3733, 29907, 235269, 578, 9606, 61926, 235265, 1417, 7154, 577, 3658, 861, 2971, 675, 573, 8727, 37132, 577, 1411, 696, 1277, 1963, 578, 798, 1707, 7704, 19424, 16011, 235265, 109, 235284, 235265, 100922, 575, 5748, 6915, 5640, 235292, 30896, 603, 20305, 604, 22522, 3779, 22241, 235269, 22488, 235269, 578, 52138, 2962, 235265, 67087, 604, 696, 3476, 235248, 235274, 235308, 235276, 4363, 576, 23135, 104941, 9987, 689, 235248, 235324, 235308, 4363, 576, 48679, 9987, 1853, 2788, 235265, 109, 235304, 235265, 3512, 3594, 6284, 235292, 31458, 3594, 3614, 6284, 603, 20305, 604, 6915, 578, 9663, 1578, 235290, 16978, 235265, 1165, 7154, 577, 41748, 18068, 235269, 4771, 31572, 1411, 235269, 578, 15093, 9606, 5115, 578, 24091, 1411, 235265, 67087, 604, 235248, 235324, 235290, 235315, 3763, 576, 6284, 1853, 3354, 235265, 107, 108]\n",
            "labels:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 07:31:56,036 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 07:31:56,037 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-12-09 07:31:56,037 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|2025-12-09 07:31:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[WARNING|logging.py:328] 2025-12-09 07:31:56,549 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1172] 2025-12-09 07:31:56,549 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2341] 2025-12-09 07:31:56,550 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:986] 2025-12-09 07:31:56,551 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:16<00:00,  8.02s/it]\n",
            "[INFO|configuration_utils.py:941] 2025-12-09 07:32:12,735 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:986] 2025-12-09 07:32:12,736 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:423] 2025-12-09 07:32:12,830 >> Could not locate the custom_generate/generate.py inside google/gemma-1.1-2b-it.\n",
            "[INFO|2025-12-09 07:32:12] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-09 07:32:12] llamafactory.model.loader:143 >> all params: 2,506,172,416\n",
            "[WARNING|trainer.py:906] 2025-12-09 07:32:12,884 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !df -h\n",
        "# !touch output/debug/testfile && ls output/debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cUT-URABqDK",
        "outputId": "1cf7704f-7f59-4423-dbb1-674f6fc8c61e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   45G   69G  40% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.7G  4.0K  5.7G   1% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "tmpfs           6.4G  4.6M  6.4G   1% /var/colab\n",
            "/dev/sda1        74G   47G   28G  63% /kaggle/input\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "testfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls output/test-lora"
      ],
      "metadata": {
        "id": "NUkmHvREASDZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"output/test-lora\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RHYpdS2BESd",
        "outputId": "69dc4853-80b0-4dc3-be78-6d303687db70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"google/gemma-1.1-2b-it\"\n",
        "adapter = \"/content/LLaMA-Factory/gemma_lora_sft_output\"   # your LoRA folder path\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "\n",
        "prompt = \"Explain what is QLoRA\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "2a14756366f048c990c569f4e96794f3",
            "8f4ac9dde7344396b76036d964da31b9",
            "7386b31891024d6596ff5f6a32fa4cd0",
            "17bdcef1f0334a4681647fbbad3f3ef3",
            "e9f054691e9545e0ba627c7b33e33c98",
            "3fe013bf54824818b01676cb5842642e",
            "518d22c6903c42988786eefe77a8774a",
            "de57a09e83c34ab2baa9abfaeea6fe71",
            "296b464e196c4b2da2acd69501410f79",
            "4ec2cbe29f6343dd927c257b909524a6",
            "171e9377a40b4663a2f85c9215dd33a5"
          ]
        },
        "id": "H46HTzcvuVTQ",
        "outputId": "33bcc7e9-bf78-4208-ff37-032c88d9b1d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a14756366f048c990c569f4e96794f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at '/content/LLaMA-Factory/output/llama3-qlora'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/LLaMA-Factory/output/llama3-qlora'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-970301169.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Explain what is QLoRA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 440\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 )\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/content/LLaMA-Factory/output/llama3-qlora'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LGRj2u2a_Uxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llamafactory.cli export train_gemma_qlora.yaml\n"
      ],
      "metadata": {
        "id": "lIWGecfpuXWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a14756366f048c990c569f4e96794f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4ac9dde7344396b76036d964da31b9",
              "IPY_MODEL_7386b31891024d6596ff5f6a32fa4cd0",
              "IPY_MODEL_17bdcef1f0334a4681647fbbad3f3ef3"
            ],
            "layout": "IPY_MODEL_e9f054691e9545e0ba627c7b33e33c98"
          }
        },
        "8f4ac9dde7344396b76036d964da31b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe013bf54824818b01676cb5842642e",
            "placeholder": "​",
            "style": "IPY_MODEL_518d22c6903c42988786eefe77a8774a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7386b31891024d6596ff5f6a32fa4cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de57a09e83c34ab2baa9abfaeea6fe71",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_296b464e196c4b2da2acd69501410f79",
            "value": 2
          }
        },
        "17bdcef1f0334a4681647fbbad3f3ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec2cbe29f6343dd927c257b909524a6",
            "placeholder": "​",
            "style": "IPY_MODEL_171e9377a40b4663a2f85c9215dd33a5",
            "value": " 2/2 [00:21&lt;00:00,  9.10s/it]"
          }
        },
        "e9f054691e9545e0ba627c7b33e33c98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe013bf54824818b01676cb5842642e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518d22c6903c42988786eefe77a8774a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de57a09e83c34ab2baa9abfaeea6fe71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "296b464e196c4b2da2acd69501410f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ec2cbe29f6343dd927c257b909524a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171e9377a40b4663a2f85c9215dd33a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
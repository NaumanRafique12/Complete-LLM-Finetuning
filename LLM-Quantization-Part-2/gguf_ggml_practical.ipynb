{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jil1Dr_ful-P"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y cmake build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTIaktJ4N2qy",
    "outputId": "199436c0-a29e-4858-e6d9-5ee79c09b534"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSVg6OQ1N3ud",
    "outputId": "09a731bd-1e02-4804-9d1f-0a5755ca505b"
   },
   "outputs": [],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFIgN3k3VMOO"
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p_-Ut0fN4nD",
    "outputId": "37681a79-b4ea-4c80-82d7-266262282b7a"
   },
   "outputs": [],
   "source": [
    "!make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443,
     "referenced_widgets": [
      "e559f6ddce0544eb959887ffbb3d82f1",
      "e3629c402dc841b5a2d73f5a4510b196",
      "97a2d95384614272a713a43391fc1fd9",
      "cd9fd422cc6048a48b8a5dc1476f5f3f",
      "5821edf2c41d4431a45ea0b53ca6a75a",
      "fca79f406e10497b85eee1926569f1c4",
      "5a64548c3bf344ee88a5d941bd14b5ea",
      "42f3b3bee9f24790b4f6438de0ae2718",
      "415f0afc8d2142698cba84712e226e06",
      "c08e2de1b80647e39560a3cc0f3ec7ab",
      "d42d0ff7ab784c7aa770f4935bf0a9db",
      "87db03c279ff4e829b9957ca8253921a",
      "f7fc01e16c624eb9965ab450fc115841",
      "470fe511d6214bdf9282bc11bafd9049",
      "cbafd5bee5dd4a18bdad2d8b3d8bb74c",
      "1b7f8c3fda5145a790bbd61d98eec90a",
      "6edd5e70ab0b4a15935aa06975f417d7",
      "eeb9079884f44510b81210fe7a982f3d",
      "77e01cf51bbf49438256def14f0a40b7",
      "38522aa943cf45fe989a0ca31573be52",
      "12ea238440294dd3b1af005f43d3a790",
      "8603d99afbd04c9d8d62843ff7095978",
      "753870ea7083401283e09a1838a877d1",
      "e1148bbe0e804ae08604c776a08e7c41",
      "527a9edb1eeb4b00984b418746c601ca",
      "0b4a883504d143f2ac0441f9a9aa0ff5",
      "98805409c8264738ae88509f6740791e",
      "74a6615d78d941929664175691835f15",
      "50b03ce87e2c461b9c7985e000f1c06e",
      "a9ba4acd1ea3427187fa8b2840581105",
      "899c9e3ffbc8461a87777870eb0545d2",
      "1aba086ae53246b7b5aa48cd6192781f",
      "c0e89974ee374c49bdfda852a9fffc13",
      "414d99ad97db4615aa3f7b59a210ee35",
      "2e1bfe6c89634b9fb60fead8162e0f39",
      "33bab874b08647f0885041400db04469",
      "5cf1f54ed2b2434e8bdc980ac6049a23",
      "cddc47c2d3be478db24d46e8fe3d9c20",
      "57a66c7f435742e8aad140fa7489b733",
      "243baaf1a7904198b2926481043775ec",
      "cdf3bf4adf314cb09be510ac2dab66f5",
      "6884ac10acb64724ac89f202364866f0",
      "3b5fe6449a134eca8c10e4caeb11c279",
      "136e128f87c64fd281545e562d4ce367",
      "842bd764b7c54d42ac7e8b6f74b2577d",
      "0d2fcf4da17140aaac554e69c4a3baa3",
      "9037ff66a0024b2b842466ba0c4dc6b6",
      "0476d8fa85034407bd535bf7095a9566",
      "fda134445447416ea4eeb85f4ce896bb",
      "1af29164ce2746759c98b5d9c40f2fa0",
      "cc452cda878144b9a09be25958784d12",
      "3cb90c40122949e6aed0d4c0b7e9990d",
      "81ed7517ffd64a889eed78dee52c1194",
      "c7cc9795a43a44748d9af78092cbe505",
      "f4dfb22869964c09a5f04a88071ae592",
      "80bd78563bf1457eb6daf63a9faea8f7",
      "4e1181616d9a4d4ea6e03b07e829e003",
      "28ccf6b6410c452390abde3f88f80f8b",
      "87a5736cd6d640b78b11ee19a2ef1c9d",
      "83c8cb394f854f539f2fe0e5d41abaf7",
      "1df2f751c7f341f49b2b0afd8a8ff482",
      "6302c0ad18464a6e89ca4aa6b71ac076",
      "2d6ea239dfb542d5bb8085f398122f03",
      "6474e717ed9f43378913587003f3a392",
      "e2fce1eb37504c1fbfe6946d3afd09ce",
      "d8ea75cd821f45029e0cb12bfb5be2f3",
      "5bc2795e1c7e48e282212984c12b1a79",
      "a8636bca737b4b4faf983719e9bcd0c7",
      "34dacfbc460e4e29af5b11f9573600bb",
      "7ebb285e266f477f9c4424d121fac87d",
      "f466d8aeba4646f5aeb9093d5cea90bc",
      "9fcc75486eb84ad5b2d69f43f85f9f90",
      "5556fdffb8894ab69bdc5df1b6706b50",
      "ef32e18050364788959755a552fea82d",
      "d9fe76f3e6dc41cf96d3b3a3e5b86682",
      "11950804306b4aaeaa83e927250899d4",
      "bbddcf71bebf45d5aeba838385504862",
      "b8648dfa91244fbcbc022d6b337b09f4",
      "7f6f0973ea5a42b8828fd10b77708b88",
      "d6fe2b0000e249cbb69360eab8cb414c",
      "bd48f4b553fb4155a15cf7a78b56928b",
      "54097b3971294744811a29a75663d3a0",
      "5a7d8371e14145bd9a189e231b7c7b11",
      "25bd0083eff54db99ea1dde29d26edf2",
      "1a6f475d91af4bb68235331b47fb61b8",
      "61fa8b48cfa644b3a23345daeb624eb9",
      "de6a36bebd2940fbbaf588b09af861fd",
      "a441a109abab4d3f93c331bff4d304c4",
      "ee6d9a394c454b829aec2c65c80f2813",
      "4345ee3b8029466aa0b8dab90959355a",
      "5ba86858d08047be929dbd21ffb70417",
      "93654ab9d2904597bc6d9e728d0be22b",
      "7ee841391c8b4d7ebdc0ecee11c3f722",
      "47c00eb7c7d6432eb5c393f96eea67e9",
      "5d21386e35854b519e9ccc7c6fdb31ad",
      "b8e49041533445dcbc187ab1e1f3713a",
      "78ea2a4845344c63bda6e8d2e90b987c",
      "880c4e976e6640e58281690d51eb808a",
      "55eda092ad4a4f5d93d4300106d44eb4",
      "dddcf91c39f54a8a9601842f76659860",
      "c9c3b7c1f9334b7387aa4c48000a0b7c",
      "9114d22b35874f07adf4b0162e46dd5f",
      "aaf9162add564fd68b3638b225d0356d",
      "5bd9001f58014c5d86dac01b00a9a9f5",
      "78b72395b320439ba610a6d3f29b0ba5",
      "8c664f3333e3404399f68daf95ff64c5",
      "c0a469caa8a84f338d2b78fb00708000",
      "c61f5f4d62d7471bb95619bb3121925c",
      "dc09c337df95445292c90f0367c37e77",
      "2bbc12b083a344a2abaee2defea70f21",
      "99066042647947cc952f2f2757b5294e",
      "af90a3dc74524fa98f25995b468469bf",
      "55509e8ddb6b47f387cd57ca78c0b921",
      "0e10c29d8536496499aa8be090f2c2f6",
      "34d09ae6e63d42e6929b8e518ddd8aef",
      "ce823944a1704d47a1655a2f3a4f4be3",
      "0459bd2d29bf42a2ae505ce12ac4c139",
      "1db6b58ef5ac474499ff93902b21aa75",
      "855e2436fb44429589ae8085529ebcab",
      "fb8e4bec7d7b4bc7a63d9ff8a5eb67c9",
      "dabeb6f1e20a4b1ebee4178cad177268"
     ]
    },
    "id": "4v5at6xmvClR",
    "outputId": "138bae87-39b9-4734-b787-ac01cc38fc89"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    local_dir=\"tinyllama-hf\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKLoqpFJPGRr",
    "outputId": "8b196782-f9ed-4bbb-d3e0-e79abbb59825"
   },
   "outputs": [],
   "source": [
    "!pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwR6cj_aO7CL",
    "outputId": "2c718ebd-c9d9-4db2-ba4e-ca66bee78bfb"
   },
   "outputs": [],
   "source": [
    "!python3 convert_hf_to_gguf.py ./tinyllama-hf \\\n",
    "    --outfile ./tinyllama-1.1b-chat.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5BFegN9WBi1"
   },
   "outputs": [],
   "source": [
    "# !./bin/quantize ./tinyllama-1.1b-chat.gguf ./tinyllama-1.1b-chat-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBLRA6RbwG44",
    "outputId": "cc8c45f8-fba8-4ac9-9eaf-0cb30a733138"
   },
   "outputs": [],
   "source": [
    "!./main -m ./tinyllama-1.1b-chat-q4_0.gguf -p \"Explain quantization in LLMs\" -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "w20J40FJx1oY",
    "outputId": "aced19fc-ac7a-4bda-a18d-8242d12a2ccf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wtD1Avhydmo"
   },
   "outputs": [],
   "source": [
    "!mkdir -p build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ti0hzrNyiVn",
    "outputId": "02b37ae0-4cca-438d-8ff5-2a8e7e333897"
   },
   "outputs": [],
   "source": [
    "%cd build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYc3HMolQfXm"
   },
   "source": [
    "!cmake .. â†’ generates build instructions (Makefile).\n",
    "Think of it as creating a blueprint for how the code should be compiled.\n",
    "\n",
    "!make â†’ follows those instructions and compiles the binaries.\n",
    "This is the actual construction step â€” turning source code into executables.\n",
    "\n",
    "Output = ./main, ./quantize, etc. â†’ now you can run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADL6kyUXzBiw",
    "outputId": "9f52688a-ccfa-482f-e4c5-566b5f63250d"
   },
   "outputs": [],
   "source": [
    "!cmake .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWpbyg4PzDB4",
    "outputId": "2700e12e-4433-485d-eba9-530da76a045f"
   },
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45t3jaJQRARd"
   },
   "source": [
    "cmake + make â†’ build the inference engine (./bin/main).\n",
    "\n",
    "quantize â†’ prepare .gguf quantized model.\n",
    "\n",
    "./bin/main â†’ run inference with your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnoLvAo61nlI",
    "outputId": "7f6f994f-91c0-4466-b7ea-f57284467ff2"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3Xm7zUlh1_xg",
    "outputId": "8875e694-9df7-4d75-fff3-6a75a44d9e31"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cT3rjdpISx5G",
    "outputId": "549f73b3-14c4-472f-8ea7-a9f40e440eb1"
   },
   "outputs": [],
   "source": [
    "!ls -lh /content/llama.cpp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFSGQt-FTZt-",
    "outputId": "c0976697-fd5d-4c4a-924c-9f75958b1985"
   },
   "outputs": [],
   "source": [
    "!./bin/llama-cli -m ../tinyllama-1.1b-chat.gguf -p \"What is quantization in LLMs?\" -n 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlItusQC2zJf"
   },
   "source": [
    "âœ… llama-cli (official main runner)\n",
    "\n",
    "âœ… llama-run (multi-prompt / batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwEkskGU2r_X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ==== Step 1: Load your document ====\n",
    "def load_documents(file_path):\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    loader = TextLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# ==== Step 2: Chunk the text ====\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# ==== Step 3: Create or load FAISS VectorStore ====\n",
    "def create_or_load_faiss(chunks, embedding_model, index_path=\"faiss_index\"):\n",
    "    if os.path.exists(index_path):\n",
    "        print(\"Loading existing FAISS index...\")\n",
    "        return FAISS.load_local(index_path, embedding_model)\n",
    "    print(\"Creating new FAISS index...\")\n",
    "    db = FAISS.from_documents(chunks, embedding_model)\n",
    "    db.save_local(index_path)\n",
    "    return db\n",
    "\n",
    "# ==== Step 4: Retrieve relevant chunks ====\n",
    "def get_context_from_query(query, retriever):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# ==== Step 5: Build prompt & write to file ====\n",
    "def build_prompt_file(context, query, prompt_file=\"prompt.txt\"):\n",
    "    prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful AI assistant. Use the context to answer the question.\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "    with open(prompt_file, \"w\") as f:\n",
    "        f.write(prompt)\n",
    "    print(f\"Prompt written to {prompt_file}\")\n",
    "\n",
    "# ==== Step 6: Run llama.cpp with GGUF ====\n",
    "def run_llama_cli(gguf_path, prompt_file=\"prompt.txt\", n_predict=200):\n",
    "    print(\"Running inference with llama.cpp...\")\n",
    "    os.system(f\"./bin/llama-cli -m {gguf_path} -f {prompt_file} --n-predict {n_predict}\")\n",
    "\n",
    "# ==== === MAIN PIPELINE === ===\n",
    "def rag_pipeline(\n",
    "    doc_path=\"my_notes.txt\",\n",
    "    gguf_model_path=\"../tinyllama-1.1b-chat-q4_0.gguf\",\n",
    "    user_query=\"What is quantization in LLMs?\"\n",
    "):\n",
    "    # Load & split\n",
    "    docs = load_documents(doc_path)\n",
    "    chunks = chunk_documents(docs)\n",
    "\n",
    "    # Embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # FAISS\n",
    "    vectorstore = create_or_load_faiss(chunks, embedding_model)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # RAG\n",
    "    context = get_context_from_query(user_query, retriever)\n",
    "    build_prompt_file(context, user_query)\n",
    "\n",
    "    # Inference\n",
    "    run_llama_cli(gguf_model_path)\n",
    "\n",
    "# ==== Entry Point ====\n",
    "if __name__ == \"__main__\":\n",
    "    rag_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tk_yCCQaSB20"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d78VhOQnSB50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl7sa9ZISB8r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJ5B7BrKSCDG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGAk4IcAZu4R"
   },
   "source": [
    "GGML (Georgi Gerganov Machine Learning) ek C-based runtime + tensor library hai jo:\n",
    "\n",
    "Low-level CPU/GPU optimized inference engine hai\n",
    "\n",
    "Mainly llama.cpp, whisper.cpp, stable-diffusion.cpp jaise projects use karte hain\n",
    "\n",
    "Original format tha before GGUF came in\n",
    "\n",
    "No Python dependency â€“ pure C/C++ based\n",
    "\n",
    "ðŸ“Œ GGUF = file format\n",
    "\n",
    "ðŸ“Œ GGML = inference engine + tensor library\n",
    "\n",
    "Practical: Run a GGML-Format LLM Model (like ggml-model-q4.bin)\n",
    "\n",
    "ðŸ§° Tools:\n",
    "\n",
    "âœ… llama.cpp (same as GGUF)\n",
    "\n",
    "âœ… Prequantized GGML model (e.g., from TheBloke)\n",
    "\n",
    "âœ… main binary from llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRpyWgwJaIPg"
   },
   "source": [
    "Step-by-Step GGML Inference\n",
    "\n",
    "ðŸ”¹ Step 1: Clone & Build llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUQz1gt8Zovd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VZevqnqaJkM"
   },
   "source": [
    "Step 2: Download a GGML Model\n",
    "\n",
    "Use any of the prequantized .bin models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzJuqbl6aDL0"
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/ggml-model-q4_0.bin -O ggml-model-q4_0.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCvOWE1FaOUv"
   },
   "source": [
    "Step 3: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuWXgdR0aL41"
   },
   "outputs": [],
   "source": [
    "./main -m ggml-model-q4_0.bin -p \"What is quantization in machine learning?\" -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSV3cvN9aWJQ"
   },
   "source": [
    "Prompt: What is quantization in machine learning?\n",
    "\n",
    "Output: Quantization is the process of reducing the precision of the weights and activations of a neural network. It is commonly used for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtnqRSZHaMBK"
   },
   "outputs": [],
   "source": [
    "!pip -q install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "# Use a small GGUF to demo; supply path to your .gguf\n",
    "llm = Llama(model_path=\"/content/model-q4_0.gguf\", n_gpu_layers=35)  # set 0 for CPU-only\n",
    "out = llm(\"Explain GPTQ vs AWQ in 2 lines.\", max_tokens=80)\n",
    "print(out[\"choices\"][0][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

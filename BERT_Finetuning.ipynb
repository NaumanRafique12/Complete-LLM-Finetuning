{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNdvv1TdSIO6",
    "outputId": "79387f75-0ded-435f-ca49-f0d281d7096b"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets fsspec transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j46yH4p_zZmo"
   },
   "source": [
    "# First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfUAIalozeSq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SZQEPvczeVs"
   },
   "outputs": [],
   "source": [
    "# take the below complex dataset\n",
    "# load_dataset(\"ag_news\")\n",
    "# load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm34eDTkzeYK"
   },
   "outputs": [],
   "source": [
    "# Customer feedback classification (positive/negative/neutral)\n",
    "# Support ticket intent detection (billing, technical, general)\n",
    "# Email/topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7imbEzlkzecj"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "# Load IMDB dataset and subset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJkLvHt1zee9"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "test_dataset = dataset[\"test\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkEOxqA2z21_"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijD42dn2z24i"
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaRZULDgz27F"
   },
   "outputs": [],
   "source": [
    "# Apply tokenization + rename + format in a single flow\n",
    "def preprocess(ds):\n",
    "    ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])  # remove raw text (saves memory)\n",
    "    ds = ds.rename_column(\"label\", \"labels\")\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eBrtHUcz29W"
   },
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFKiKAIiz-GC"
   },
   "outputs": [],
   "source": [
    "test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERFLeSvwz-8P"
   },
   "outputs": [],
   "source": [
    "# 3. Model load\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tldMs-8z--1"
   },
   "outputs": [],
   "source": [
    "# Classifier head trainable rahega by default\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False  # Freeze BERT encoder\n",
    "\n",
    "# Unfreeze last 2 encoder layers\n",
    "\n",
    "# for layer in model.bert.encoder.layer[-2:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0TN9vYUz_BJ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-finetuned-imdb\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cR8gys70Yi6"
   },
   "outputs": [],
   "source": [
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQhrExLJ0bpQ"
   },
   "outputs": [],
   "source": [
    "# 6. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBmgg3Pn0dMr"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aLbbW4x0dPV"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2aJKe_h0e8j"
   },
   "outputs": [],
   "source": [
    "# 7. Evaluate\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y3Wug-n0fAk"
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmJq0Nwz0hs3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsQZuD860jps"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/content/bert-finetuned-imdb\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"/content/bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5UF9Zzh0jsN"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-5cIQ3a0mjp"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "text = \"This movie was amazing and I loved the acting!\"\n",
    "result = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1Q9PAln0mmC"
   },
   "outputs": [],
   "source": [
    "print(result)  # Example: [{'label': 'POSITIVE', 'score': 0.98}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdev1wUG0q2h"
   },
   "source": [
    "### Pushing it to Huggingfacehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLeh1DNI0qOA"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iDLXdrf0qQd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QblwcgII0vUC"
   },
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"sunny199/my-bert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP7qOhFE0x54"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"sunny199/my-bert-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3WGIzCWzXC7"
   },
   "source": [
    "# Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KouthQeTBiw",
    "outputId": "4c6a621d-325f-4f34-df28-2587e0b0243d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast\n",
    "    BertForSequenceClassification,\n",
    "    BertForTokenClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3agIb3Na8KO"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBhaZgqPTDWm"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaWtdUXTHPj"
   },
   "outputs": [],
   "source": [
    "# BERT Text Classifier\n",
    "class BERTTextClassifier:\n",
    "    \"\"\"BERT for Text Classification (Sentiment, Spam etc.)\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_classes\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_imdb_data(self, sample_size=5000):\n",
    "        \"\"\"Load IMDb movie reviews dataset\"\"\"\n",
    "        print(\"Loading IMDb dataset...\")\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "\n",
    "        # Sample data for faster training\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                       min(sample_size, len(dataset['train'])),\n",
    "                                       replace=False)\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                      min(sample_size//4, len(dataset['test'])),\n",
    "                                      replace=False)\n",
    "\n",
    "        # Convert numpy.int64 â†’ int for indexing\n",
    "        train_texts = [dataset['train'][int(i)]['text'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['label'] for i in train_indices]\n",
    "        test_texts = [dataset['test'][int(i)]['text'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['label'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_texts)}\")\n",
    "        print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "    def train(self, train_texts, train_labels, epochs=3, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the text classifier\"\"\"\n",
    "        train_dataset = TextClassificationDataset(\n",
    "            train_texts, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def evaluate(self, test_texts, test_labels, batch_size=8):\n",
    "        \"\"\"Evaluate the text classifier\"\"\"\n",
    "        test_dataset = TextClassificationDataset(\n",
    "            test_texts, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        report = classification_report(true_labels, predictions,\n",
    "                                     target_names=['Negative', 'Positive'])\n",
    "\n",
    "        return accuracy, f1, report\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"Predict sentiment for new texts\"\"\"\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        self.model.eval()\n",
    "        for text in texts:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "                predictions.append(pred)\n",
    "                probabilities.append(probs)\n",
    "\n",
    "        return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfvPhWd5Tz2s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"Dataset for Named Entity Recognition\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_list, labels_list, tokenizer, max_length=512):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels_list = labels_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        labels = self.labels_list[idx]\n",
    "\n",
    "        # Tokenize with word-level alignment\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get word alignment for batch index 0\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(labels[word_idx] if word_idx < len(labels) else 0)\n",
    "            else:\n",
    "                aligned_labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Ensure aligned_labels length matches max_length\n",
    "        if len(aligned_labels) < self.max_length:\n",
    "            aligned_labels += [-100] * (self.max_length - len(aligned_labels))\n",
    "        elif len(aligned_labels) > self.max_length:\n",
    "            aligned_labels = aligned_labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Shape: (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Shape: (max_length,)\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long)  # Shape: (max_length,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WYdrwaT1Mv"
   },
   "outputs": [],
   "source": [
    "class BERTNERClassifier:\n",
    "    \"\"\"BERT for Named Entity Recognition (CoNLL-2003)\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=9, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Label mapping (CoNLL-2003)\n",
    "        self.labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n",
    "                       'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "    def load_conll_data(self, sample_size=2000):\n",
    "        \"\"\"Load CoNLL-2003 NER dataset\"\"\"\n",
    "        print(\"Loading CoNLL-2003 NER dataset...\")\n",
    "        dataset = load_dataset(\"wikiann\", \"en\")\n",
    "\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                       min(sample_size, len(dataset['train'])),\n",
    "                                       replace=False)\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                      min(sample_size//4, len(dataset['test'])),\n",
    "                                      replace=False)\n",
    "\n",
    "        # Convert numpy.int64 to int\n",
    "        train_tokens = [dataset['train'][int(i)]['tokens'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['ner_tags'] for i in train_indices]\n",
    "        test_tokens = [dataset['test'][int(i)]['tokens'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['ner_tags'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_tokens)}\")\n",
    "        print(f\"Test samples: {len(test_tokens)}\")\n",
    "\n",
    "        return train_tokens, train_labels, test_tokens, test_labels\n",
    "\n",
    "    def train(self, train_tokens, train_labels, epochs=3, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the NER model\"\"\"\n",
    "        train_dataset = NERDataset(\n",
    "            train_tokens, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def evaluate(self, test_tokens, test_labels, batch_size=8):\n",
    "        \"\"\"Evaluate the NER model\"\"\"\n",
    "        test_dataset = NERDataset(\n",
    "            test_tokens, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                # Collect valid predictions (ignore -100)\n",
    "                for i in range(preds.shape[0]):\n",
    "                    for j in range(preds.shape[1]):\n",
    "                        if labels[i][j] != -100:\n",
    "                            predictions.append(preds[i][j])\n",
    "                            true_labels.append(labels[i][j])\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        return accuracy, f1\n",
    "\n",
    "    def predict(self, tokens_list):\n",
    "        \"\"\"Predict NER tags for new tokens\"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        self.model.eval()\n",
    "        for tokens in tokens_list:\n",
    "            encoding = self.tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt',\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "\n",
    "                # Get alignment for original words\n",
    "                word_ids = encoding.word_ids(batch_index=0)\n",
    "                token_predictions = []\n",
    "                previous_word_idx = None\n",
    "\n",
    "                for i, word_idx in enumerate(word_ids):\n",
    "                    if word_idx is not None and word_idx != previous_word_idx:\n",
    "                        if word_idx < len(tokens):\n",
    "                            token_predictions.append(self.labels[preds[i]])\n",
    "                    previous_word_idx = word_idx\n",
    "\n",
    "                predictions.append(token_predictions)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noGChIBBUvKx"
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset for Question Answering (SQuAD-style)\"\"\"\n",
    "\n",
    "    def __init__(self, questions, contexts, answers, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer = self.answers[idx]  # dict: {'text': [...], 'answer_start': [...]}\n",
    "\n",
    "        # Encode inputs with offsets to locate answer\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")[0]  # (max_length, 2)\n",
    "        start_positions = torch.tensor(0, dtype=torch.long)\n",
    "        end_positions = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        if answer and 'answer_start' in answer and answer['answer_start']:\n",
    "            answer_start = answer['answer_start'][0]\n",
    "            answer_text = answer['text'][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Find token start/end matching answer char positions\n",
    "            for idx, (start, end) in enumerate(offset_mapping):\n",
    "                if start <= answer_start < end:\n",
    "                    start_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                if start < answer_end <= end:\n",
    "                    end_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                    break\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),      # (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXlml3TfVDVH"
   },
   "outputs": [],
   "source": [
    "class BERTQuestionAnswering:\n",
    "    \"\"\"BERT for Question Answering (SQuAD)\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_squad_data(self, sample_size=2000):\n",
    "        \"\"\"Load and sample SQuAD dataset\"\"\"\n",
    "        print(\"Loading SQuAD dataset...\")\n",
    "        dataset = load_dataset(\"squad\")\n",
    "\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                         min(sample_size, len(dataset['train'])),\n",
    "                                         replace=False)\n",
    "        val_indices = np.random.choice(len(dataset['validation']),\n",
    "                                       min(sample_size//4, len(dataset['validation'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        train_questions = [dataset['train'][int(i)]['question'] for i in train_indices]\n",
    "        train_contexts = [dataset['train'][int(i)]['context'] for i in train_indices]\n",
    "        train_answers = [dataset['train'][int(i)]['answers'] for i in train_indices]\n",
    "\n",
    "        val_questions = [dataset['validation'][int(i)]['question'] for i in val_indices]\n",
    "        val_contexts = [dataset['validation'][int(i)]['context'] for i in val_indices]\n",
    "        val_answers = [dataset['validation'][int(i)]['answers'] for i in val_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_questions)}\")\n",
    "        print(f\"Validation samples: {len(val_questions)}\")\n",
    "\n",
    "        return (train_questions, train_contexts, train_answers,\n",
    "                val_questions, val_contexts, val_answers)\n",
    "\n",
    "    def train(self, questions, contexts, answers, epochs=3, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train QA model using QADataset (offset mapping based)\"\"\"\n",
    "        train_dataset = QADataset(questions, contexts, answers, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def answer_question(self, question, context, max_answer_len=30):\n",
    "        \"\"\"Answer a single question given context\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "            # Ensure valid span\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "            if (end_idx - start_idx) > max_answer_len:\n",
    "                end_idx = start_idx + max_answer_len\n",
    "\n",
    "            # Decode predicted tokens\n",
    "            answer_tokens = input_ids[0][start_idx:end_idx+1]\n",
    "            answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "            return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBxYZ0B7VTNX"
   },
   "outputs": [],
   "source": [
    "def run_text_classification_demo():\n",
    "    \"\"\"Demo for text classification\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT CLASSIFICATION (Sentiment Analysis) DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    classifier = BERTTextClassifier(num_classes=2)\n",
    "\n",
    "    # Load data\n",
    "    train_texts, train_labels, test_texts, test_labels = classifier.load_imdb_data(sample_size=1000)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample Review: {train_texts[0][:200]}...\")\n",
    "    print(f\"Label: {'Positive' if train_labels[0] == 1 else 'Negative'}\")\n",
    "\n",
    "    # Train for 2 epochs (small for demo)\n",
    "    classifier.train(train_texts, train_labels, epochs=2, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1, report = classifier.evaluate(test_texts, test_labels, batch_size=8)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Test custom examples\n",
    "    custom_reviews = [\n",
    "        \"This movie was fantastic! Amazing acting and great plot.\",\n",
    "        \"Boring and terrible. Waste of time.\",\n",
    "        \"Not bad, could be better though.\"\n",
    "    ]\n",
    "\n",
    "    predictions, probabilities = classifier.predict(custom_reviews)\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "    for text, pred, prob in zip(custom_reviews, predictions, probabilities):\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        confidence = prob[pred] * 100\n",
    "        print(f\"'{text[:50]}...' -> {sentiment} ({confidence:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ3FV7p3Vo7G"
   },
   "outputs": [],
   "source": [
    "def run_ner_demo():\n",
    "    \"\"\"Demo for Named Entity Recognition\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NAMED ENTITY RECOGNITION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    ner_model = BERTNERClassifier(num_labels=9)\n",
    "\n",
    "    # Load small subset for demo (fast training)\n",
    "    train_tokens, train_labels, test_tokens, test_labels = ner_model.load_conll_data(sample_size=500)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample tokens: {train_tokens[0][:10]}\")\n",
    "    label_names = [ner_model.labels[l] if l < len(ner_model.labels) else \"O\" for l in train_labels[0][:10]]\n",
    "    print(f\"Sample labels: {label_names}\")\n",
    "\n",
    "    # Train for 2 epochs\n",
    "    ner_model.train(train_tokens, train_labels, epochs=2, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1 = ner_model.evaluate(test_tokens, test_labels, batch_size=8)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Custom examples\n",
    "    custom_sentences = [\n",
    "        [\"John\", \"Smith\", \"works\", \"at\", \"Google\", \"in\", \"California\"],\n",
    "        [\"Apple\", \"Inc.\", \"was\", \"founded\", \"by\", \"Steve\", \"Jobs\"]\n",
    "    ]\n",
    "\n",
    "    predictions = ner_model.predict(custom_sentences)\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "    for tokens, preds in zip(custom_sentences, predictions):\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Labels:\", preds)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Fx5D5uVpki"
   },
   "outputs": [],
   "source": [
    "def run_qa_demo():\n",
    "    \"\"\"Demo for Question Answering\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUESTION ANSWERING DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    qa_model = BERTQuestionAnswering()\n",
    "\n",
    "    # Load small subset (for speed)\n",
    "    (train_questions, train_contexts, train_answers,\n",
    "     val_questions, val_contexts, val_answers) = qa_model.load_squad_data(sample_size=500)\n",
    "\n",
    "    # Safe sample print\n",
    "    ans_text = train_answers[0]['text'][0] if train_answers[0]['text'] else 'No answer'\n",
    "    print(f\"\\nSample Question: {train_questions[0]}\")\n",
    "    print(f\"Sample Context: {train_contexts[0][:200]}...\")\n",
    "    print(f\"Sample Answer: {ans_text}\")\n",
    "\n",
    "    # Train model (2 epochs for demo)\n",
    "    qa_model.train(train_questions, train_contexts, train_answers, epochs=2, batch_size=4)\n",
    "\n",
    "    # Test on custom questions\n",
    "    print(f\"\\nCustom Q&A Examples:\")\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is the capital of France?\",\n",
    "            \"context\": \"France is a country in Europe. Paris is the capital and largest city of France. The city is known for the Eiffel Tower and the Louvre Museum.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who founded Apple?\",\n",
    "            \"context\": \"Apple Inc. is an American technology company. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. The company is known for products like iPhone and Mac.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        answer = qa_model.answer_question(case[\"question\"], case[\"context\"], max_answer_len=30)\n",
    "        print(f\"Q: {case['question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "q1s2cTD6VzOe",
    "outputId": "20b674a1-99c7-49ff-ee13-eb93204bef58"
   },
   "outputs": [],
   "source": [
    "print(\"BERT Multi-Task Demo\")\n",
    "print(\"Choose a task to run:\")\n",
    "print(\"1. Text Classification (Sentiment Analysis)\")\n",
    "print(\"2. Named Entity Recognition (NER)\")\n",
    "print(\"3. Question Answering\")\n",
    "print(\"4. Run All Tasks\")\n",
    "\n",
    "choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "\n",
    "try:\n",
    "    if choice == \"1\":\n",
    "        run_text_classification_demo()\n",
    "    elif choice == \"2\":\n",
    "        run_ner_demo()\n",
    "    elif choice == \"3\":\n",
    "        run_qa_demo()\n",
    "    elif choice == \"4\":\n",
    "        run_text_classification_demo()\n",
    "        run_ner_demo()\n",
    "        run_qa_demo()\n",
    "    else:\n",
    "        print(\"Invalid choice! Please run again.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n--- ERROR OCCURRED ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"1. Installed required packages:\")\n",
    "    print(\"   pip install torch transformers datasets scikit-learn tqdm numpy\")\n",
    "    print(\"2. Loaded all classes & dataset helpers (BERTTextClassifier, BERTNERClassifier, BERTQuestionAnswering, TextClassificationDataset, NERDataset, QADataset)\")\n",
    "    print(\"3. Using the fixed versions (with int casting, offset_mapping for QA, word_ids fix for NER)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w0hFz1ssV7VG",
    "outputId": "3bd3cc94-a8cf-45e4-ac97-4f08b819d259"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"https://huggingface.co/datasets/eli5/conll2003-parquet/resolve/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"https://huggingface.co/datasets/eli5/conll2003-parquet/resolve/main/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"https://huggingface.co/datasets/eli5/conll2003-parquet/resolve/main/validation-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "683e15b492494a8f964d7055b6646d0b",
      "afff9f1dfa61480a9c76150237fff8b4",
      "80c444cbc0004ca5bafd98185b814883",
      "f5a16c2f714045a89582b874f43712ad",
      "856fb64f6a2a4a5c9c2bfc1c5b5b6985",
      "4edc7e741eb149e8a297db3e5fbf660e",
      "03d13600aeb34fffbd36def3b2203484",
      "f5ab94bf544e40f693346858f1a2d873",
      "2173f76cfc1b43fe8111e1aeabbbf5c8",
      "080d7298baff4fc69a8c0f8d7c5dd99e",
      "bce498586ccb40d7a0dfc5c12600b228",
      "92118d50312947ca81ed1f23ded976b0",
      "847d177affa34884bddd33af81f4d884",
      "5d1517cbc93a4bf2a90478d0240b0a27",
      "6ce3a43161be4cb29efeaa9876291679",
      "a0838d7eb0244c2aa3060979863dd36f",
      "bc93e29416474c619bb754acfed789eb",
      "74909aa2588744b0ad19fac8d64bd58a",
      "55ea7d8ef45944c994eb155a57c039e8",
      "e1e44c9882d5471289499f23ff4cda37",
      "08a429d38d7e4b18838abff9d4bf932b",
      "a48f101dae2f41538683247a9281a9e7",
      "0157dcd37fd4430c93c80474e307ec74",
      "977cc789dc9145859f99f061e492e974",
      "67a14444c95f4775b546d5c7b4161d11",
      "9f3b31e546aa46c687c47ab84b18ae58",
      "6d920c3bf6d34f0798522273945713b8",
      "4c9684c9203a4d759e744df6aa868876",
      "dca2a018fadd4d2eafa273d03dec92ab",
      "8cfa1a85abb5455aaa1deed3c8a05dd1",
      "930b3b640b644073bbcf0f80009a0d84",
      "3324bdf4898e49e0846b922c0be7cc55",
      "7bc86573d76445c4a008902b95e83a97",
      "8e668628afcc4c13bb43ad0fcf3fe563",
      "5793bff34ff8426ba04e9a4d68cf76dc",
      "9c67ba79b3f94565b9a5dd0bbf6f22de",
      "e48d0b698c3249a88483e4473c0729d7",
      "d46ef972e20f447fa69e4f4b624cd558",
      "383b2c66e3fb4efe80e7d23107405c49",
      "43dac5c812434055b3a99e6d374c078c",
      "749da63d4bf442ba97d5161baeeaad57",
      "019ab3e5a11f479b82eb47993ec3dcac",
      "812af9efe5414a7f89b9cb35a28a6cf6",
      "ea257680b57b4386a0df055b8a0468c1",
      "3af976f2e4e74de3a9a26020d43ab7e2",
      "d6dd87bc10c94236a0a09d9bc9c087aa",
      "57634a189ba14d5699f5cfd203450a91",
      "f4405498e4cb49a7bf45c7e664aeb64e",
      "0d0e157efe21497e9644a0bba0c317a2",
      "9d5f0cb13e744fc79e1b13478a074bff",
      "eea42c7f4dfe4660800d374427dc44fd",
      "2e56510bf7334c6a91c25a49cb66d902",
      "895f4f066c734f00b6caaabc02a319f6",
      "ebd84d0e90b64c4eb6507b9ddaa14e39",
      "51177694d5b34f65af503c48d964dd45",
      "808e93bd5c4e4e7dbd58decffd8b8aa6",
      "6b37a444cf584191a8ea664d19f19b24",
      "0f713ffa80914e06a040f9724d78c701",
      "5f5159abda3046589199e6adde58cd5d",
      "ebe2fcb53d7c42b48036e7da1e53370c",
      "342b99be67ee42b1a66fc90ffd145fcc",
      "0c25fc103d2147ab88c234d47eadaf59",
      "4072e2c5356345d9b71bb9092592dae3",
      "035165ac52794c71874a05da7beec4e1",
      "c6d0f85e863d41139cd8dd4be30d60d4",
      "2c2094c525814647991ab8ba976e6433",
      "398451be9b1a42329bb1b37e5ccf348b",
      "f8d78848d862415393a1983def9e5e00",
      "691acb4d62654b1c801d2b3480b9e22f",
      "4a261ea9678c43d1a4b82fa441da884f",
      "f4354b08410f4a398fff3672bf641219",
      "7e236a03f5b944dc894bf5b952bf5c8a",
      "639d2d0bd5984de79dd9ff047d1da1fa",
      "bc433b28c06746609f2446fc7b152056",
      "4f048afa4163423cbc9adc37c63e8803",
      "a41b8cd9a474410da0a4f025e37515cf",
      "6bc345427c22434e894dc52184c10221"
     ]
    },
    "id": "VJAM8LhAaMCG",
    "outputId": "b88f4399-5076-44a9-9fa8-0cbffbeb780e"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikiann\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfXDa53caeZO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

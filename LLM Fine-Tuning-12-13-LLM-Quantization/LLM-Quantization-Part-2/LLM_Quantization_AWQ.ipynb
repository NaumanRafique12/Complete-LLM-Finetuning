{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vu7PODo8C-9S",
    "outputId": "b4a9fe56-24ba-4230-db03-7b7b37f2402c"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U autoawq transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlLrUZZCqlm0",
    "outputId": "a013ac98-d550-4fd0-a498-7d73eecbd7d9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTDTXEqcqtZz",
    "outputId": "f0479e55-1a74-4fac-b750-8c03babff9c3"
   },
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(\"Checking PyTorch and CUDA versions...\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsfX59M6uPfc",
    "outputId": "560a1adf-9320-424b-ecef-f53c208ede19"
   },
   "outputs": [],
   "source": [
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "d521a34baa304846b980d5cbaa04652d",
      "fe9a033891574989a4bd8430137c8530",
      "4884e698dfb14da5a67382cb734400e5",
      "7c29f07fe7be4c3390b0f460d493ad68",
      "2e4099fa620145938839f9a25d002ba0",
      "d9ed675138294b749fb2568fab20b495",
      "28ef8d55f7ec4bf08d9d702bee0ad2a7",
      "8b19f81e27b54cc9b9971737948d4971",
      "83913f22b96b40af85cbe1a28a68aaad",
      "52b571dafe104815be23742e72c2b946",
      "d89feacdc62a42eba798908cd33cbb14",
      "88c06defe1f44574815f2ed22fc15692",
      "aab97b52ea794f6c9feca4478eac2548",
      "fac4346bc8064b7fa3ce0d1131a09394",
      "96d68b3a589441fbb2074d716b33dad3",
      "b0dc0b98caef4e609bce7a49ffd9a4ae",
      "2c6d984fb4b44fc09984632ffea68d75",
      "0c85d904ae064c4d959362ba47ec02c1",
      "30bc97b76f3d4da989c8818ed2e1ce02",
      "3870857092644f328ab003779bf8edb8",
      "ec9c2c5600c348ae969a803dfc04e637",
      "7ab13ae05c2b4eb69c4605301a4c202f",
      "f166e61fcdfb4ff9861f1051e6546c18",
      "9c6d98c81ebf48908a791b16fd516f81",
      "9f8ad51853564905999feaafe75a800b",
      "5252fe4d0b234886a46c2163508cbd27",
      "1388b94635554db0a775806f490e3c2e",
      "05a7d269b4a14391a5cf2def83be609e",
      "ddce851772274564b69ef2ced9f1c1cd",
      "348abbca2e7247bd9e1796654bedd6e6",
      "d7acde4ed1814dd89888f54168cd7f0c",
      "64714fe289c74b319faa05a9f765c479",
      "9ca700add27b45668a10098b628171a7",
      "ffe471691cd7488ca324a699c674d2b1",
      "f7089c9f34c944d19fd92553626a3eb3",
      "9742ea18d31446bea697f33fbcee72e6",
      "0cc33599ff614971a6c0a133e10ff622",
      "6934a0d6710f46b8a3446e5030a83ba9",
      "6bbfe84039f74c03b01fd4d80cd1ac7e",
      "840ab709b40b44abae7e117b33ae08ea",
      "94e26c19852a43399560ed0273e7985e",
      "436eea463418474a8323a383cedfc5eb",
      "ea188ac843fa40b793a411e0f5e10db1",
      "41cdc452784b4e47bba6645a3ff0ff1a"
     ]
    },
    "id": "fNwuBoqbuS2d",
    "outputId": "c6c6a1d2-5b14-4ac9-a711-bc97153260ab"
   },
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "ddd703cd26a84306aed598f12fb47b8d",
      "8e06d01da0e94f2183d61027316a7510",
      "5d604c8f316d494693693327606b8b9a",
      "b32ab45143ae40e88093d4b9116d265c",
      "dae30325d2f44eff9e6beac7a1d6959e",
      "0f922ccbb85e441b80ce106de242906d",
      "b5f70b2e7fdf4e7483160b1cab40f73b",
      "84d4619db6684d4ab37aea8c39ea55c9",
      "e4ad431dc7d34ef59ca165e21868d14f",
      "b8f014331d3f4aa382bf0151c86d8002",
      "b08fec133da14d978f7820adb91c65ae",
      "3f33ea8026254b1696a37be08876e215",
      "be1a95210beb48e994bd0e372d928fa2",
      "f10ecb30c8a841c38d7338027d19349c",
      "d0caf9cdec234b4091ee348c5450f401",
      "83d098cc3bf848d39cdf944d77939df9",
      "9dd7684cc1584b03a424a431a5603fa7",
      "612b8be521f0440da39cd95ddba5e964",
      "4b71de1138d44ac9a7f708ce27a753a2",
      "bec790a54c4548809ecc396c6668b9dd",
      "9318684595a24b6ca119496581b3f0ca",
      "518719462f884524870a53e058a882d9",
      "c3ee469f28e94513a51808f6449dedbe",
      "d06bbd53bf1c48438e6de2aa9b546f54",
      "8c1c4de9884942ca817e5373f494417a",
      "6b57d4dc92e545918394610c6a1411cd",
      "5fa3145096e1491f97a0088dfa68e65b",
      "214a0d5dbef84c8599cc8547772370cb",
      "8ba95b0eea974a0abeeea174d21a9894",
      "47057b33b6cf44a39bf2416452fdf0be",
      "a010bcdeb83041d1af511ff52c741d68",
      "2c15eb945f234472b16e9fb6d2fed581",
      "043f375995d64c8da485e75c273fee87",
      "6d8c1fdb397f4c63a63c6cf934a61244",
      "cab043591c864ec496f0321ddc4cc19d",
      "b5a701e5c10c4569ac576330fabf9fb9",
      "5324e7519d9c40ffb99740e51a69dee9",
      "e0204ff9d8814cf3b8f7b32b931dfc94",
      "4139ecb8193144929611e277dacec0c5",
      "ae2c473cd9d8402ca4ebfacd883a756a",
      "d69f130655d046d5a4892ac5c2894059",
      "8ae729ec0a28495fa926fb5dc43e50de",
      "5294ff3926d5439985e767623a46e629",
      "73fcb8b68c6f4273a9b98655992d2b52",
      "608c5676a6e34f32a263fc00d2bbba32",
      "990deb144b114178a873d2ae35aa747c",
      "d42b08d907ce42ce8564a9c963354aac",
      "e7d8aac0171c4d558bee13bc3178f781",
      "b9dfb08bd297474782c4edd76bde872c",
      "a6e20a4c1d814ed6a25acb43d3530297",
      "c1a3829f53c746ed9c79002e0473f08b",
      "80cffbc1bc97430a8ee3b2ba7b4c959c",
      "4c2bceb17b4f420eb0ac715e318a6a13",
      "c0f644f5dc7a4de3b625b6785218f457",
      "d13cfeddfe29456cbe6b377e4fc4b9ed",
      "e27b3140115140beb4ba87bc287098a7",
      "b9dbf2c60d4542349c52c3cb2fa01242",
      "c6f1bdaabef544779909df87c10a7a67",
      "50c3634220094f8599601ec5afd07ee4",
      "d59438dead504ab98f7ccd4fc8ff8830",
      "eaea38a92f3e44ab8675a79288235e59",
      "d11188ed349a43a4b9dba646e4144970",
      "1b9420bb4b974a9dbb007e108f3b1114",
      "89c3064fa0254a798eda01f123e9aec0",
      "6a427fd3f8fe43e892e0ac53b81655df",
      "60dd6ab9c84e4217a2d5fbb4c2af0249",
      "7a6b76f20bf54e04a61afe8d59dbb724",
      "b9dd2acf444a4de5899905d8015fda20",
      "4f97d7bfcbf54ec58849bf6ab0abd6ad",
      "8e7947d095484924a7b242fed816eb8b",
      "8b2bc0f49f27405f9590a50c5581f479",
      "6f089eba163a48e5a5a3c2f6aa727b10",
      "65dda14b1a644258b680f90b378af39e",
      "6c744c7e5241497e851296a2fb39a64b",
      "6bd5f84174654389896cef0fd8d9aab9",
      "297b3a23a88c45d6af67563b826c7245",
      "d7a881c8a0ee42819bed087c1dea037c"
     ]
    },
    "id": "jHPfLaMpuXkV",
    "outputId": "d37ab161-e126-4c32-ad0c-063f88fe74ac"
   },
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "# Load model with specific configurations to avoid attention issues\n",
    "mdl = AutoAWQForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  # Use fp16 explicitly\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else \"cpu\"  # More specific device mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRNyvLNeIF20",
    "outputId": "d89063d6-d377-458d-9d2e-1ede2d814b60"
   },
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "mdl.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZGU4NTSu5GQ"
   },
   "outputs": [],
   "source": [
    "# AWQ quantization config\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7wpOIX2u8z3",
    "outputId": "d304ac45-8ae5-41af-b604-22aa4ef639a7"
   },
   "outputs": [],
   "source": [
    "# Create more diverse and shorter calibration data\n",
    "print(\"Preparing calibration data...\")\n",
    "calib_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning models process data efficiently.\",\n",
    "    \"Natural language understanding is advancing rapidly.\",\n",
    "    \"Deep neural networks learn complex patterns.\",\n",
    "    \"Artificial intelligence transforms technology.\",\n",
    "    \"Computer vision recognizes objects accurately.\",\n",
    "    \"Robotics integrates sensors and actuators.\",\n",
    "    \"Algorithm optimization improves performance significantly.\",\n",
    "    \"Data science extracts meaningful insights.\",\n",
    "    \"Software engineering creates reliable systems.\"\n",
    "] * 10  # 100 samples total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMh4r_mEKetx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_USE_SDPA\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDqrd-F_KlIN"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "cfg.attn_implementation = \"eager\"   # fallback attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZYnt6RZKqPg"
   },
   "outputs": [],
   "source": [
    "calib_tokens = [\n",
    "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
    "    for text in calib_texts[:50]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "4MIV2MMdKt6M",
    "outputId": "afea3b67-ffe3-4517-cbe6-411f3c5b1557"
   },
   "outputs": [],
   "source": [
    "mdl.quantize(\n",
    "    tok,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calib_tokens,\n",
    "    max_calib_seq_len=128,\n",
    "    max_calib_samples=50,\n",
    "    n_parallel_calib_samples=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "9LCM9JzOK3ow",
    "outputId": "fdcef33c-30e2-434d-dd66-2800e34b24a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_USE_SDPA\"] = \"0\"   # Disable SDPA attention\n",
    "\n",
    "calib_tokens = [\n",
    "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
    "    for text in calib_texts[:50]\n",
    "]\n",
    "\n",
    "mdl.quantize(\n",
    "    tok,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calib_tokens,\n",
    "    max_calib_seq_len=128,\n",
    "    max_calib_samples=50,\n",
    "    n_parallel_calib_samples=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "9mCOWzxzLJSs",
    "outputId": "f112128c-e03f-4e70-ecc3-36fdf6be749e"
   },
   "outputs": [],
   "source": [
    "calib_tokens = [\n",
    "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids[0].tolist()\n",
    "    for text in calib_texts[:50]\n",
    "]\n",
    "\n",
    "mdl.quantize(\n",
    "    tok,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calib_tokens,   # now proper format\n",
    "    max_calib_seq_len=128,\n",
    "    max_calib_samples=50,\n",
    "    n_parallel_calib_samples=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "AfkDLulQLL4m",
    "outputId": "85f5267e-a1c0-4f51-e2d2-385f78a3ac4d"
   },
   "outputs": [],
   "source": [
    "mdl.quantize(\n",
    "    tok,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calib_texts[:50],   # raw text list\n",
    "    max_calib_seq_len=128,\n",
    "    max_calib_samples=50,\n",
    "    n_parallel_calib_samples=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "FutaFE1xIOP5",
    "outputId": "7461545c-2d36-488e-9d20-a67b4ba8d210"
   },
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "out_dir = \"tinyllama-1.1b-awq\"\n",
    "print(f\"Saving model to {out_dir}...\")\n",
    "\n",
    "mdl.save_quantized(out_dir, safetensors=True)\n",
    "tok.save_pretrained(out_dir)\n",
    "\n",
    "# Save config\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "config.save_pretrained(out_dir)\n",
    "\n",
    "print(f\"Model successfully quantized and saved to {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6267074d505247ac8133e71cc2dc1cc4",
      "dfd6a8442f48478b933c786a9d0ceec1",
      "a5f4571246d24b28a00925921f87b3ef",
      "14c8f12c6e3146058a43f4c730ac7ce1",
      "ce25f38d6da04fd8a86a5925b41b5e09",
      "2f5b172136e341d886b0016b3baf185e",
      "f624b9a8e4a945d6ba61a8d378c10585",
      "e2d6a51bbe06462ba81de0cbc1d39f2f",
      "c78b786744f74575adfc1aff7a23c66e",
      "ae95ca44602c44b3b6edca638bc63d4e",
      "282191c3427745aba9b2bd7d792aaf6c",
      "dd011f7cbc6544cbbaf9dc6eafe6ecf1",
      "ffe365cc87824786a765d49b2847d7f1",
      "0897ed4cd3ba41bb80eaebb852d10961",
      "7af68a2c88a14b58a2b20e7fc9dc0ce0",
      "c350fb2eebd64f4a9d396982a250fedb",
      "588589507ca142e580dabe7f9ff4583a",
      "88fcf2a044f24fb4833a00e692f1c75b",
      "c7ff88a541204eecb738082b7a936787",
      "bbc6da65166a4a4d9020f7d3eeffd608",
      "64b2f18fa69d40a8912a9145915cf0cc",
      "6b806e98dced4ab8a38800cefa3b242d",
      "3663d44ceb4c4a91a3f1f6150d767c62",
      "00c808a5e82b43cdbf7f1f7e103dbb99",
      "0f29de3806754d67bc19485a3d18457d",
      "c7c6ee62629c4e7abd273a33dab1056b",
      "1c72935452fe4ae2b1da9577f090cfaf",
      "444411a5b15b4f85939f473d17e7680e",
      "eaed8985a0ac44fbb23631186395c6e8",
      "681de2d320064e369b6a514a4a197c2c",
      "fd41870da50546c681e01143f020df6c",
      "4ef22ecfb1734bfb80769f92aad144c3",
      "ab7f000bfa584ddf8b58ebfcc900f89e",
      "8ea3a4709c024b1b9b20e1317b06c946",
      "bdd68a2ca3c943d4bf657933fea12256",
      "c6a3293d97d542d89b2f9f4f85827993",
      "a3bfdde5c4084a048b7444f42d8f94e0",
      "102371f9a8a64f49909577c7e456ed90",
      "cbeaa2ce438240c9853e7e893fb21da6",
      "bb28dd8f66eb46b7a424f4498281d1ad",
      "3f29e2425c4746ad95a6863167093446",
      "66414c734d4c44aeacecfbec07e37457",
      "163137c493f94806b1af5a606efe6c37",
      "5ce7e82ecfcd411b8a7044bb038b09f2",
      "347fd8d000b7419f9e5b28946e8777d9",
      "0b038f6f7b394fba82693bd57d7c9cb5",
      "43b9d2cc2cbc46fa8d169fa2f2a2cbfd",
      "d8ea1711fb8540cf9b7702ecffba4c3f",
      "597d8598342f4175b524a2aa91116cf8",
      "290c49e3a6c8479499d9a058368f1c0d",
      "e5e5fe1f8bf94010ace32b05072f6d36",
      "c409956f06da46f08610992eb3dd30b9",
      "59cf97164c8c40bcb794d1489679ec52",
      "60d45e49212241bea8e8c3c7246332be",
      "634f027e78834c97bdfd13d8bd5f5e1e",
      "68dbf41716a34847aae25791cd75d714",
      "0c58183715314abb97e087210516c7d2",
      "0efcfb9d403942e1ad586577354a7249",
      "de0c0cb872584dedb604d0a38302959f",
      "7006222fc8fa47cca901be133b525ae0",
      "fa223d3b7d8d4745a3499ef4d2dd25d9",
      "568f03b054564eb8bc0c24954321ecc7",
      "454e461f412c4cbf8e2a47949a7f2b41",
      "04bb171c06a748a08fd128be9b292b32",
      "990ef601537c4d2c90305497a1bb7f39",
      "6c420258b82241e485a0bf8218365852",
      "942016f6eda049529c04ccc003256a0a",
      "240e9fcdb73d4e3784f19c68dfd6881d",
      "0bafcaefe3474be2a1875e4f4db9e8c0",
      "eebf7575ffeb4f00906142101f0596cd",
      "08a2297ac62f4437abadeba5ebb42727",
      "77f8c1f824d847a5a4b4322409001de1",
      "adcdc3c238164263af3c5d41bccbc40f",
      "712cebc4100c491bab612747be8cb320",
      "95f36103ef2f47beb791e08cb2782e12",
      "3108cd9a064141169a424476c098c7b4",
      "5f0dc1fd8079431f9d6d2a81af85144a",
      "b8f634c470e04c8996058931f93e6094",
      "c5939abedba0405597f75c7b90069fb8",
      "49941e3cf0a948e5832c26a045c6dad3",
      "201689c12ec240fe83d77abc27898e36",
      "b9c7c96ce94d48488c3a7f6a8f7e4eb6",
      "914174af689343e390532f7a40cd33f8",
      "1254d97d0a30491da5695e262e22d25d",
      "a796962bd2434139b6506599da0df83f",
      "6b1402267e274d4eaf6526865c318d44",
      "2bc33356406b4167b72023412e4e3e99",
      "11a2e1ca32e04b5c9ebff658d62b8c0e",
      "1d29ea32f10e449586709f5bb07e0521",
      "82a286051798477aaafb9d5db79da80e",
      "914682c5b40a48a0b8206dc9e8bedd89",
      "ab8d3893f4ca403f9a5174ba2b459786",
      "54af642cc57248dabb06266d7764b733",
      "fd46bd32896c49c08632e65a726e2b05",
      "ba7c7125ad5c476c859aeeb5d676a07e",
      "785d2db3e6ef42bfb11a62fd8a9b6182",
      "d50448b201ff4b4498c1b119fe263c92",
      "87cbfb7ae9574079ac55f96b01a7c736",
      "ab94319a6b8646158ccbb3e2abf551bb",
      "39d076e85dbb4cabb3ff3ef79b960640",
      "2eca90f4e04c43c0b83ca87f7357d0c0",
      "0ccda797e5924b65866fee67c8be6db4",
      "09568d19867849d49304b2a50abdb9d5",
      "1124bb0bf0244fd58e77c9f892a5506e",
      "73c2cc7744f249b9802d176203139f60",
      "cad6df2130a143b08620128e7a9e7cf4",
      "8648597ac2e040e3bb9a16644f3cfa1e",
      "e6f6faeeaaba4e43b0f883e64eeab1d6",
      "b71dd821e66f4f44ab5dd9fc1a7fd2c6",
      "60d90e5efc744bc9b6f803051932c1d2",
      "063de1e5d8394b1da217f18d03d59e6e",
      "5e5f26140fd0445d92e130902fea5f28",
      "3465814ae0af4892930c6dd845f95e36",
      "10a0854a2ca84fe283fa4af4a4e4f7ec",
      "35acf099de254e2187cd219d61a97b27",
      "e04ed384ee604a50a5f6ea7401164fc4",
      "664c7b3cd3134b6587a9aa600f251e38",
      "7560542cb41c4494a791ed2a7bd43629",
      "f7d404f9dff44155831c868664e7b82e",
      "b333c9fb69c94279b3aa283c51037944",
      "90ecf414b00b4569b36fd13bec16e987",
      "2532ff333c5f494f8a171e139b11b190",
      "d1b3f14b59f449c2a0f4444e0ac18948",
      "651880e5f716453c986fc50bf928b039",
      "fed6df8d03dc46ccbf45e91e40e409ba",
      "bfe2afc6733b45afaf98a308e2995be2",
      "9f10c87ec9944af3a0c52ba70102b762",
      "5f3197f3517b40d7866aa5e7bb1f29a7",
      "433b64ad0dbc4fe59d1a87a965de56bf",
      "ad6e2759acbe4c3da86bccbfe438d488",
      "a6b8c90c017447438b53da6a4931f573",
      "828dc77ea615412a8be8b21b12786d43"
     ]
    },
    "id": "Qy-hTtLNc6Wh",
    "outputId": "61e82ed5-400f-4541-8d37-70d067c58d9d"
   },
   "outputs": [],
   "source": [
    "print(\"Starting quantization...\")\n",
    "try:\n",
    "    # Use minimal configuration to avoid batch size issues\n",
    "    mdl.quantize(\n",
    "        tok,\n",
    "        quant_config=quant_config,\n",
    "        calib_data=calib_texts,\n",
    "        max_calib_seq_len=128,     # Reduced sequence length\n",
    "        max_calib_samples=50,      # Reduced sample count\n",
    "        n_parallel_calib_samples=1 # Keep sequential processing\n",
    "    )\n",
    "\n",
    "    print(\"Quantization completed successfully!\")\n",
    "\n",
    "    # Save the quantized model\n",
    "    out_dir = \"tinyllama-1.1b-awq\"\n",
    "    print(f\"Saving model to {out_dir}...\")\n",
    "\n",
    "    mdl.save_quantized(out_dir, safetensors=True)\n",
    "    tok.save_pretrained(out_dir)\n",
    "\n",
    "    # Save config\n",
    "    config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "\n",
    "    print(f\"Model successfully quantized and saved to {out_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Quantization failed with error: {str(e)}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    print(\"\\nTrying alternative approach with different model loading...\")\n",
    "\n",
    "    # Alternative approach: Load model differently\n",
    "    del mdl\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "        # Try loading without device_map first\n",
    "        mdl = AutoAWQForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        # Move to device manually if CUDA is available\n",
    "        if torch.cuda.is_available():\n",
    "            mdl = mdl.cuda()\n",
    "\n",
    "        mdl.eval()\n",
    "\n",
    "        # Try with even smaller calibration parameters\n",
    "        print(\"Attempting quantization with minimal parameters...\")\n",
    "        mdl.quantize(\n",
    "            tok,\n",
    "            quant_config=quant_config,\n",
    "            calib_data=calib_texts[:20],  # Only use first 20 samples\n",
    "            max_calib_seq_len=64,         # Even smaller sequence length\n",
    "            max_calib_samples=20,         # Minimal samples\n",
    "            n_parallel_calib_samples=1\n",
    "        )\n",
    "\n",
    "        out_dir = \"tinyllama-1.1b-awq\"\n",
    "        mdl.save_quantized(out_dir, safetensors=True)\n",
    "        tok.save_pretrained(out_dir)\n",
    "\n",
    "        config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "        config.save_pretrained(out_dir)\n",
    "\n",
    "        print(f\"Model successfully quantized with alternative approach and saved to {out_dir}\")\n",
    "\n",
    "    except Exception as e2:\n",
    "        print(f\"Alternative approach also failed: {str(e2)}\")\n",
    "        print(\"\\nConsider using a pre-quantized model instead:\")\n",
    "        print(\"TheBloke/TinyLlama-1.1B-Chat-v1.0-AWQ\")\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sRw8NNXLqA5"
   },
   "source": [
    "The script runs an AWQ quantization pipeline with a safe, low-memory fallback path.\n",
    "\n",
    "Primary flow:\n",
    "\n",
    "Calls mdl.quantize(...) using your tokenizer, AWQ quant config, and calibration texts (prompts).\n",
    "\n",
    "Uses conservative settings to avoid OOM: max_calib_seq_len=128, max_calib_samples=50, n_parallel_calib_samples=1.\n",
    "\n",
    "On success, saves the quantized weights, tokenizer, and model config to out_dir.\n",
    "\n",
    "If quantization fails:\n",
    "\n",
    "Frees memory (del, torch.cuda.empty_cache(), gc.collect()).\n",
    "\n",
    "Reloads the model with low_cpu_mem_usage=True and use_cache=False (and moves to GPU if available).\n",
    "\n",
    "Retries quantization with smaller calibration settings (shorter seq length, fewer samples).\n",
    "\n",
    "If it still fails, suggests using a pre-quantized AWQ model.\n",
    "\n",
    "Role of prompts (calibration texts):\n",
    "\n",
    "They provide short, representative inputs so AWQ can observe activations and pick good weight scales.\n",
    "\n",
    "More and more realistic prompts ⇒ better quality after quantization; your code uses a small set to keep memory low.\n",
    "\n",
    "Important parameters:\n",
    "\n",
    "max_calib_seq_len & max_calib_samples: control calibration length/size (quality vs. memory).\n",
    "\n",
    "n_parallel_calib_samples: set to 1 for minimal VRAM use.\n",
    "\n",
    "low_cpu_mem_usage=True, use_cache=False: reduce RAM/GPU memory footprint during load/quant.\n",
    "\n",
    "safetensors=True, trust_remote_code=True: safer format and allow custom model code.\n",
    "\n",
    "Gotchas / prerequisites:\n",
    "\n",
    "Ensure variables & imports exist: mdl, tok, calib_texts, quant_config, base_model, plus torch, gc, AutoConfig, and AutoAWQForCausalLM (from the AWQ library).\n",
    "\n",
    "Don’t mix GPTQ configs with AWQ; use the right quant config for the backend.\n",
    "\n",
    "If you hit OOM, reduce max_calib_seq_len/samples further or increase group size.\n",
    "\n",
    "After quantization:\n",
    "\n",
    "Load the saved model directory with the AWQ loader (from_quantized) and run generation as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqiL6VH8c6wC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

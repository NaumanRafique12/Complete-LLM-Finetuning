{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYPMXa55ipbx"
   },
   "source": [
    "GPTQ = original algorithm/paper (ETH Zurich, 2022).\n",
    "\n",
    "AutoGPTQ = practical Python library (Hugging Face maintained) that made GPTQ mainstream and easy to use.\n",
    "\n",
    "https://github.com/AutoGPTQ/AutoGPTQ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGnAe6UG43Ba"
   },
   "source": [
    "| Feature / Library       | **AutoGPTQ**                          | **bitsandbytes**                                      | **gptqmodel**                                                  |\n",
    "| ----------------------- | ------------------------------------- | ----------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| **Origin**              | Hugging Face ecosystem (2023)         | Tim Dettmers (2022)                                   | ModelCloud (2024–2025)                                         |\n",
    "| **Quantization Method** | GPTQ (post-training, 4/8-bit)         | Linear quantization (INT8, NF4, FP4, 8-bit optimizer) | GPTQ v1, GPTQ v2, QQQ, EoRA, GAR                               |\n",
    "| **Target Use**          | Easy Hugging Face integration         | Training & inference memory savings                   | **Full production deployment toolkit**                         |\n",
    "| **Hardware Support**    | CUDA (Nvidia GPU)                     | CUDA (Nvidia GPU) only                                | CUDA (Nvidia), ROCm (AMD), XPU (Intel), MPS (Apple), CPU       |\n",
    "| **Integration**         | Hugging Face Transformers             | PyTorch Optimizer + HF integration                    | HF Transformers, vLLM, SGLang, Optimum, Peft                   |\n",
    "| **Inference Kernels**   | ExLlama, Triton, Marlin (via plugins) | cuBLAS-based INT8 kernels                             | Marlin, ExLlama v2, Torch fused, BitBLAS                       |\n",
    "| **Training Support**    | No                                    | Optimizer states (8-bit Adam, NF4 LoRA)             | Partial (LoRA + EoRA fine-tune on quantized model)             |\n",
    "| **Flexibility**         | Focused on GPTQ only                  | Training + inference memory efficiency                | **Dynamic per-layer configs, adapters, GAR, eval integration** |\n",
    "| **Evaluation**          | None built-in                         | None built-in                                         | Built-in `lm-eval` & `evalplus` hooks                          |\n",
    "| **Ease of Use**         | Very easy (HF-style API)            | Very easy (drop-in optimizer / load\\_in\\_8bit=True) | More advanced config, but still has high-level API             |\n",
    "| **Community Models**    | Huge (TheBloke, HF Hub)               | Many LoRA + finetune models                           | Growing rapidly (ModelCloud & HF Hub vortex/EoRA releases)     |\n",
    "| **Strengths**           | Easy Hugging Face usage               | Simple + effective for training                       | All-in-one production toolkit, multi-hardware                  |\n",
    "| **Weaknesses**          | Limited to GPTQ only                  | Nvidia-only, no GPTQ                                  | More complex, newer ecosystem (still maturing)                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC2kYd0yOqm3"
   },
   "source": [
    "Versions & Techniques\n",
    "\n",
    "GPTQ v1, GPTQ v2 → GPTQ ke alag implementations / improvements\n",
    "\n",
    "QQQ → Quick Quantization for Transformers (speed optimized variant)\n",
    "\n",
    "EoRA → Efficient Online Row-wise Approximation (better per-row error handling)\n",
    "\n",
    "GAR → Gradient Aware Rounding (quantization ke liye advanced rounding strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00n9egXp46Dq"
   },
   "source": [
    "https://github.com/ModelCloud/GPTQModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKecg9Fw4q_x"
   },
   "source": [
    "pip install gptqmodel → install the gptqmodel package.\n",
    "\n",
    "-v → verbose mode, so pip prints more details about what it is doing (downloads, builds, etc.).\n",
    "\n",
    "--no-build-isolation → disables pip’s default build isolation behavior when installing from source (sdist). It means pip will not create an isolated environment to build dependencies; you’re responsible to have all build dependencies present already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AvzUv_8kiyIo",
    "outputId": "0d4af2fd-867e-4585-f577-72ae4995ad0f"
   },
   "outputs": [],
   "source": [
    "!pip install -v gptqmodel --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAzww8Jr5Mw3",
    "outputId": "97cb8088-20ae-476a-c4aa-015fdbd3a23c"
   },
   "outputs": [],
   "source": [
    "!pip install \"protobuf<6.30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQrqnBUYi31p",
    "outputId": "547269ad-0334-4d57-9f63-fd78592f7a06"
   },
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8XKNrhux5I9",
    "outputId": "f98e1340-dd15-4879-e050-cb2ee2bbdc48"
   },
   "outputs": [],
   "source": [
    "import gptqmodel\n",
    "dir(gptqmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYETuNOu5_b_"
   },
   "source": [
    "https://huggingface.co/collections/ModelCloud/vortex-673743382af0a52b2a8b9fe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774,
     "referenced_widgets": [
      "048768a4049d4b60bd1754f438712970",
      "e2679318ca0049adab0d167bb190a72c",
      "205038732ef542d49c69f1909a9d5b1b",
      "08dc90ae5f994872b23a4e79dc951fb7",
      "8f7d284716374b2faa746e8479f79a36",
      "84dfbddda5b94917939ed835d2f68be2",
      "2975b1a1ca964bf9a2c4a383b977bed1",
      "25dba08e2c2a4c4eab929363ea700118",
      "e957fba17c854f7db6ceb3d486645b5f",
      "4c0759b7bdb2465ca72002ca78884aac",
      "5486905706a84d87b1f9c5f9d2d1ea60",
      "ef6e42abe60245748c923061eb46c99d",
      "f38257ae76cb4d129c70ade88e898828",
      "67f5752733034c49b58388b4bef24467",
      "e6725874f5f74e37a79ba27ef6be2b51",
      "64ea85447395457ca918be26a2f0d983",
      "5870f50a132445cda43791fb4def9501",
      "c0e303fa856d48399bb386edd948da4c",
      "3ea01e1fb0404ddd86b42ff99a6fa09e",
      "a180e3f7c149431ca73d0f8c32e88e6e",
      "a6c846ff04d141eab70922500209ff34",
      "0750a8637a484b2e9804f533eb69856b",
      "06ec7bd7561c48c197d7a24674fd8fa3",
      "3b08dae18a934310a7f4fe0d21bb7b63",
      "9a70cfd269884d5cad4398a145a4757c",
      "e28d78d86e094364bfeee74ebbdfe0b1",
      "66fb454825aa46bbb3189bcf4e245cb4",
      "da26f908fd974898ab8818c285c2f49d",
      "d7cbecb38d0e4ed4a0c548639618c7f3",
      "af74927f508046cfaa155dc24a02966c",
      "f9446b13a2d0458d820b4911c6b49279",
      "d588e2e3978648418a209e71030b11dc",
      "376a2565075a4a13b4feab5935544957",
      "8ef078107e704aa08e0733b70f927643",
      "566262cf397f4c5e8eb3c0027beaee7d",
      "50c2dbb470ee46aaa8d71cbc92fe7027",
      "f4a4c6c6cb4941e3940bf70abc79bed4",
      "70e256e7a8e8457d9516c352a3c2e215",
      "da14e5cc27d9401c97a0814274688195",
      "8fe4da959474463b81c43fd5a61f0d53",
      "764cf60808e745ae84f5dbf94bb9ef0d",
      "aa0cbfc2fabb4004ac3e8cd6321fff8a",
      "27b402a603724758a95d73c174d228ce",
      "a25e5b14ae654a7195228885f674e8b1",
      "22601f946f8840c9bcb27d9c512baeb0",
      "2028f97c2a0d4dbaadee40d326285da8",
      "2423bab6c99e4302924d7a33322e9906",
      "b5be3e1dc04545858f80e546bf1010a5",
      "4f319f0b5ab1413890340cd7251c4ebd",
      "231f821b8df34d6492bc817c02a0af48",
      "6ba133ef497a4468bf766f7765b046a9",
      "e769f847cf974f76b785afa9327e47dd",
      "e1d917d76a964f3aa550e4b7f36c3a8f",
      "80465ff1e67547f49b17db81ace1d672",
      "ef6bc6242b2c46af95423ad91160492b",
      "0b047e3f0d2843af9b8f9595aaf0e3c8",
      "d1a2e5d3eb494174aa9544d2697a6290",
      "5584215d1f62477f8fc8e9d98b9f9ff4",
      "e7a06b1cbfc54aadbc901e0cf35e11b2",
      "2f36dc7e92834fa096e333bc1a0b8afc",
      "5c20bef903864ccab1082724f54f8906",
      "68b7a065efd145b581037ae6a0450f60",
      "aff8a3c20e5c46c4b27d19538301ef0a",
      "b89519eb66884455a5e1c49c62bacc43",
      "80be98a3ad734ca281606d42c9047ed1",
      "da45224ad97f4de5ae8ba25bc05ae539",
      "e33ed756c7524d57805beb23c16e4512",
      "2e028b15d53b4085baae39fa0235e832",
      "b4fe7e306b634baeb5bc4d5e2613a930",
      "8aa4519942504b21bb4a5ad5f522cef6",
      "1feb486d70f140e796e734de94b7c1e2",
      "cc04c95707b344cd8c4752b16ba3286d",
      "782b1a914bf24571bd0984f11dabc749",
      "8a157ebbd9b34140b6ae820d57d1c30a",
      "5fe9c116c5eb49a18fd49084940c5606",
      "0ceee0b9f30840ba80972359b68518f9",
      "f674b62073d649c78e33d7f46a5e3c81",
      "4e88726d4a4c43938069c81c03e5b939",
      "4a3d6b64b51248da8f9a41655c1ee93f",
      "6e27f7c7cb744545b56ddfdda551b70e",
      "b0cf5279dbe74798acf8b57591cf0ed9",
      "c7387aeaa3f74c12b8617e1cd4068632",
      "2666cf52260c480a8cb0911d0e7a5111",
      "e1b57198aff0499ea57b0778082f661b",
      "cf942e2ad7bd4c2a86d139916dabc0a6",
      "3824189fdc3c4675b832f8926c77dddb",
      "f5f028a3145b42079c5bee42758be1ea",
      "ad3ba095e3e64860b77ef39c4a84c8cc",
      "66d7c06c003748af97bb95deaf73a5e0",
      "9e14869e8f474b6c876eac4428d97db2",
      "a3755b2c731245aa97b36c136fb3c428",
      "4a0fb2fcb9bf45ebaa4daa5126a05607",
      "24f1537acc1748ba98ac605d6969407b",
      "30e7196221274f1892464c2030e589f8",
      "4085223e72fa441e8648fdfbee913311",
      "54e29df5cc29431f9cfd839371301c78",
      "5446498ab26944d2a6d6044840a0b393",
      "6a0b4ebc8da945d2a95ba5c6dbc09c4b",
      "d906b592cd6e446a894d40404b565bba"
     ]
    },
    "id": "h1Sb8ook5hM7",
    "outputId": "4eb827b2-9420-4a13-f1ae-5651dd449e4e"
   },
   "outputs": [],
   "source": [
    "model = GPTQModel.load(\"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9ni531h6EVd"
   },
   "outputs": [],
   "source": [
    "result = model.generate(\"Uncovering deep insights begins with\")[0] # tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEoHdrMgjPHs",
    "outputId": "11c88a8d-7b73-4a89-dfd2-572bb53b5850"
   },
   "outputs": [],
   "source": [
    "print(model.tokenizer.decode(result)) # string output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afYg85UpUFGB"
   },
   "source": [
    "<|begin_of_text|>Uncovering deep insights begins with a deep understanding of the underlying principles and concepts that govern the behavior of the system in question. In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GclAJE-GkoSq"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq5IFsz-kpl-"
   },
   "outputs": [],
   "source": [
    "quant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0uUpTAUkr5m"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    "  ).select(range(1024))[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQfVL-6S6q_i"
   },
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel, QuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g1JzZyVk9My"
   },
   "outputs": [],
   "source": [
    "quant_config = QuantizeConfig(bits=4, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736,
     "referenced_widgets": [
      "f18057f45a5d4ae487fba48f1915ad85",
      "0ff39622ff5e4b3cae29525ed52a7994",
      "2e1037b9f9c34efea84168c184a250f0",
      "bde2d0e94089443bbc0108c12ae9ca56",
      "6a7375212a0148ae8288087ff856cda8",
      "172bd0a9180041c5adb9882910319c50",
      "b24861cf89c340368f5ce9f7b7d3928b",
      "522506f4b1c9407c8aeb7246d5acf7c8",
      "c894fd429b0d4a87b0005dd244faa9ea",
      "d1b3a80976ed45c180e0ddd226d4c23e",
      "2f88b5f69137437883e546b677b444ac",
      "7fb257865348430286b555c7b5216e44",
      "9677572c14d74cd380f9c9c783262e9a",
      "ef5c6e03f07a46dba5cc83b40506afc6",
      "630488a4a8834e83a043721ab386e479",
      "dc273be00dcd4743a181580ef513c8ac",
      "51cc8504b9aa4095af5760f914af1175",
      "6c5ecab27f074dd1b87472d5f66d604b",
      "77c1cd3630fc4c2084fdd9d8085f3e7b",
      "86fd8834bbe94a3799941010b1d59b32",
      "4765607ffe3a4270b763c805250d7af2",
      "b6ebf880970140759321e130a579327a",
      "d342b457b6e54e37873336a115035913",
      "0c42f6c378e742229f6711439eb4bea3",
      "0659c057fbde4bd6aa4d69b18f126c3d",
      "98462e0412f844aaaa41cfc708d34591",
      "a0b137c672ff400594ea99a3f9754b18",
      "65124f79547c47fc903fa147c8fe13bb",
      "5234bdbddcbc4f34919eb030972d19a8",
      "94e7c43dff4b47db8d053d206e69091a",
      "7123d8f92f814df4a1d55dcf1454eadd",
      "0eb46635b9cb4043936e5c7b1541af91",
      "1f6c42638dae40fb896d1db5381a840d",
      "160bb685b03f4a9ea196caca0afcb671",
      "a758604586c54e3493f31810afca00e3",
      "5604fab1fefe4fcfa378cf58cbc2e511",
      "0850aca53f2e4429914a53edd95c948a",
      "8f08eb0fb84345789a66656d53f331a5",
      "f42a08ef86e04e5e92695799b3bf3a4f",
      "21311c59d1314ae88db7f77a3ce6142c",
      "4be4edde52514a07bf564f918f5f796e",
      "bf66aeba9d8c4167a8e14942d0fac443",
      "7e6d5f0c3f30477b837ca14dc4c451c3",
      "78e756d5226b4060a60e25833dcb7d23",
      "560eb333fe5d442a9a159b1c14fd7a3f",
      "9d7cad2f9fa34fe4954e8a4229e63657",
      "c7e784a1117a4dcf9066cba996b755ab",
      "8ec30b7fad804290b61728e666d484c6",
      "2a570842fca04bf39952ced0c1a9ba49",
      "93e4b8a79a604d69a658bd361472c716",
      "9e1a5e10eb9143ff8e4945169c4e93ca",
      "f556b31685d94a9fa824073f2004eadc",
      "ae78468cdf594e4bb7662be4f94b2e4d",
      "aaf646000a884f8abd5db3b1a960ae4f",
      "bcd4478c103e482397c6191f8de1421b",
      "063b510df01c4ebcb8a81c79c3cb6973",
      "07caaaeb933b474d951e718f1e83f841",
      "4670d13a11e341aaaf21ae7812004e63",
      "78a0f08086f74e1c821a3c558dd9023e",
      "8b58c5df073e4c5ca55e9ff3a6389a36",
      "cee390acebde4a3cb3c21f95173cadc8",
      "c7ba02ea3d11488f839fd5df0c7bece8",
      "e27797a3d8054f48b8efd22d05a5cd84",
      "f3e28524c09643bfb45ca08246bcc87a",
      "7cffc1212e5544dfa5bb445d94e69d7b",
      "32439cb76b4d46f5ae0862810486ba75",
      "a9079ef64cab43839fc4267b303b2020",
      "a4acf8f8d50248ee9b17c92bfdda19b7",
      "ad7932d83ed34992a368979d2e4f719a",
      "a6aa95c3a5414823a1b15a067633cdbb",
      "1b482c26d089428a9a8e8ffa9c251f0b",
      "8a9b76f05eba4157bd75c8fa95bee5fb",
      "9a03abeee6f4483a932ba2aac7497167",
      "8ba84685c65d4fcc82a0b8f002da6ae7",
      "39ed3084615c4acda5fdb4450c72893f",
      "0104c40ea0b54ee38aa05f9a673d9cfd",
      "129bb3d4c8c24caf8255ed54affb6b33",
      "d3114aaea7344af4a1d5bce0a6e315fc",
      "61b72fbba75b43e3a6935a47e651a082",
      "6b62ab675f914d9f9527729d60ff9f2c",
      "837522d0f32c4c5b99215f90f28f9946",
      "cbd56de8312c47228a446888f1429574",
      "cc49b639d19f4c1c942c1d71d718af1c",
      "ce9c94e0a3a249f3930ba7811803ac17",
      "abe36ec0d7a045129d0142a4bbcbff17",
      "42854b53afc348468268767418188839",
      "eeaed0f8a28747869311ea78467ceb32",
      "42b33904df90466d8f65fcc5e8f3b245",
      "788c862c116d4c3aa65aecc20146d172",
      "7ffa6d93be054a38af3be91b25f9eebb",
      "fabca6404fdf41c2be4df87af6555e86",
      "081710dd80334a4db51106519ac44856",
      "cda3b8b09a634d8894d15f0f75f96447",
      "5842b9e2dfb04f03a66f1ac6d1734806",
      "3d4ab625be194fb6b2cdd7df216637bc",
      "b204b9880eb0434ca8efcbf7681ecd13",
      "fc1fea49b08b42a8897ea12543673a17",
      "7a8bbbc9cf3446afa9ba5ea26bfd4ed7",
      "db081d1266c449ce9d946b2ab64364ea",
      "ea8d56d2789243a4a5982a18daf1ca57",
      "4a243e0df6754710a7f8485aa8fdb1de",
      "ff68d44ac70646de8797433a476c960a",
      "e62c99f9e9cb41c9a5c59be232ffb83d",
      "ebbd5834d4b64414bafd0e785efde0d7",
      "3c61bff328ec484392f1c5161886d8c0",
      "968d78beaa694479973a95e328fc9065",
      "83ef0696b62644b4b705d1cc33b6734a",
      "3cbfe7f9566d4bd29a64de6af551fd00",
      "21470ac76d194914996c47d3c60b430d",
      "b986c530bcbe470a95ce9ff96d095d63",
      "7c8717901229494888abe15f8fe229e7",
      "dc0a5983583f4bcaad8d89115b5056c2",
      "1826f3c4a24e44a18b75ccc5b2f63602",
      "67984520dce44109a9197adbb30b452c",
      "d3c76c8476194840a5539cc767b7a6f1",
      "2379467760ca42fabd55d039ac568226",
      "d6151c8766b248ceb431eb50b72b9e20",
      "ecb07aadc7de4855949270db57047ea9",
      "f3618d62a6a041e89e18328769e953c7",
      "a229a5b6ccc84e738cc5dadaa001e990",
      "81d2f25cc3914e8981ac1086c5beb62c",
      "0a656bd3b8524ec0908d8a43665844fe",
      "39cdb8ad182449d69ec0d8b22c3f74f0",
      "1d959c66cc76491c82b23b8533cefc2c",
      "4675ad30a2db433c97430f2fe128f0dc",
      "e11966cecf494eabbac39fabb0c8ebf3",
      "7ce9fef3ab2647b5ba70c9c9aa7d7447",
      "d01c559bbbda4ed8bfaacb6181434f85",
      "26b5ecdec24643acb7e6481fb809f5d7",
      "a9897bdf14d94618bda989d06c1623a6",
      "267f0227d0a84eef873e3d034c044571",
      "5d351fa253314544b5f723775e2830e4",
      "c913e36005ea435096d4070fb2b5f1ad",
      "a1d26921f5624ef0a1d2a97269d650e6",
      "8563a63459b94d4d9c04c9c782e49129",
      "789ecb11db6947aeb55d08fc0f3ff9a2",
      "cb7d30850b4f4f32b2b9aac59f432377",
      "717f55d6768f4bffb4ae46970a82cd67",
      "471a9a84f6d14912887d93a93ef28721",
      "f6002feac89347929b3898b27116008b",
      "245f41e535d0454bb69f4f605d4e3f3e",
      "25cadcc92a9f4a3ea178e32a9c88c71c",
      "67df3009a33f419380e029b250e608ba",
      "ee72bf42cc2b40c59d2ebe2f15e82e86",
      "cd7d89556873440bb539a1314df3c2c9",
      "0db16875ff944f378b8250f2cebbe0fc",
      "0b28c68aaa56462784308f61d310d256",
      "d53bab0774a04d7b8f124ef62482980a",
      "6cea9eedaf354914bb3e6ac98c320e1e",
      "5bec7e960d754bf0a7b4e9b7addf9f63",
      "ca177015665b4c0fa6e5b5a9a08e8412",
      "fe1e1092e2ee41c686637b92d5f185d7",
      "16ba4c5798f24f95b5b374aa22eaa037",
      "0b0e067ed4344a48a367d157857a3125"
     ]
    },
    "id": "MlKaeFPOk_SC",
    "outputId": "ddfafe42-895c-48ea-ab98-f611a812c2d5"
   },
   "outputs": [],
   "source": [
    "model = GPTQModel.load(model_id, quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrjeDiMp7BVl"
   },
   "source": [
    "QuantizeConfig Parameters Explained\n",
    "\n",
    "bits: int = 4\n",
    "Number of bits for quantization (2, 3, 4, 8). Lower bits = more compression, less accuracy.\n",
    "\n",
    "dynamic: Dict[...] | None\n",
    "Allows per-layer overrides. Example: quantize some layers with 8 bits, others skip quantization.\n",
    "\n",
    "group_size: int = 128\n",
    "Number of weights grouped together before quantization.\n",
    "Smaller = better accuracy, but slower. Larger = faster, more compression.\n",
    "\n",
    "damp_percent: float = 0.05\n",
    "Used in Hessian damping (numerical stability). Prevents division by very small numbers.\n",
    "\n",
    "damp_auto_increment: float = 0.01\n",
    "If quantization fails for a layer, damping value is automatically increased by this amount.\n",
    "\n",
    "desc_act: bool = True\n",
    "Whether to use activation ordering (descending importance) for better accuracy.\n",
    "If False, disables this reordering.\n",
    "\n",
    "act_group_aware: bool = False\n",
    "(GAR feature) Group-aware reordering. Preserves activation sensitivity per group.\n",
    "Improves accuracy for grouped quantization.\n",
    "\n",
    "static_groups: bool = False\n",
    "If True, fixes the grouping instead of dynamic grouping.\n",
    "\n",
    "sym: bool = True\n",
    "Symmetric quantization (weights centered around zero).\n",
    "If False, asymmetric (different zero-point for positive/negative).\n",
    "\n",
    "true_sequential: bool = True\n",
    "Forces strictly sequential quantization (layer by layer).\n",
    "Safer but slower.\n",
    "\n",
    "lm_head: bool = False\n",
    "Whether to quantize the final LM head (output projection).\n",
    "Usually skipped for better accuracy.\n",
    "\n",
    "quant_method: QUANT_METHOD = GPTQ\n",
    "Which quantization algorithm to use (GPTQ, GPTQv2, EoRA, QQQ, etc.).\n",
    "\n",
    "format: FORMAT = GPTQ\n",
    "Output format (GPTQ, Marlin, ExLlamaV2 kernels, etc.).\n",
    "\n",
    "mse: float = 0\n",
    "If set > 0, enables MSE minimization during quantization for extra accuracy recovery.\n",
    "\n",
    "parallel_packing: bool = True\n",
    "Enables multi-threaded packing of quantized weights for speedup.\n",
    "\n",
    "meta: Dict | None\n",
    "Extra metadata for custom configs.\n",
    "\n",
    "device: str | device | None\n",
    "Device to run quantization (cuda, cpu, mps, etc.).\n",
    "\n",
    "pack_dtype: str | dtype | None = torch.int32\n",
    "Packing dtype (int32 default, but can be int16 in some kernels).\n",
    "\n",
    "adapter: Dict | Lora | None\n",
    "Allows LoRA/EoRA adapters to be applied during or after quantization.\n",
    "\n",
    "rotation: str | None\n",
    "If rotation quantization (RQ, QRQ) is applied.\n",
    "\n",
    "is_marlin_format: bool = False\n",
    "If set, stores quantized weights in Marlin kernel format (fast inference).\n",
    "\n",
    "v2: bool = False\n",
    "Enables GPTQ v2 quantization (better accuracy, more VRAM required).\n",
    "\n",
    "v2_alpha: float = 0.25\n",
    "Extra parameter controlling Hessian approximation in GPTQv2.\n",
    "\n",
    "v2_memory_device: str = \"auto\"\n",
    "Controls where GPTQv2 Hessians are computed (cpu, gpu, or auto).\n",
    "\n",
    "mock_quantization: bool = False\n",
    "Runs a fake quantization pass (for debugging/testing without actually quantizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "SNubqwJLrK3A",
    "outputId": "5f41f93f-92a1-4cc2-ca8b-281573b570c0"
   },
   "outputs": [],
   "source": [
    "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
    "model.quantize(calibration_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIQ19rzbj8zJ"
   },
   "outputs": [],
   "source": [
    "model.save(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B26AEbS6prsn"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn1Ki6kOsXLJ"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMHMt4oUsZaz"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "upload_folder(\n",
    "    repo_id=\"sunny199/Llama-3.2-1B-Instruct-gptqmodel-4bit\",\n",
    "    folder_path=\"./Llama-3.2-1B-Instruct-gptqmodel-4bit\",\n",
    "    commit_message=\"Upload GPTQ quantized LLaMA-3.2-1B model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWRuzk4-uGPE"
   },
   "outputs": [],
   "source": [
    "# test post-quant inference\n",
    "model = GPTQModel.load(quant_path)\n",
    "result = model.generate(\"Uncovering deep insights begins with\")[0] # tokens\n",
    "print(model.tokenizer.decode(result)) # string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XV8Zmbw07rPi"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "import torch\n",
    "\n",
    "def calculate_avg_ppl(model, tokenizer, texts, max_length=512, batch_size=8):\n",
    "    \"\"\"Compute average perplexity over a list of plain text strings.\"\"\"\n",
    "    from gptqmodel.utils.perplexity import Perplexity\n",
    "\n",
    "    ppl = Perplexity(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_path=None,\n",
    "        split=None,\n",
    "        text_column=None,\n",
    "    )\n",
    "    # Pass list of text strings\n",
    "    return ppl.calculate_from_texts(texts, max_length, batch_size)\n",
    "\n",
    "def load_normal_model(model_id):\n",
    "    return GPTQModel.load(model_id, quantize_config=None)\n",
    "\n",
    "def load_quant_model(quant_path, device):\n",
    "    return GPTQModel.load(quant_path, device=device)\n",
    "\n",
    "def main():\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    quant_path = \"Llama-3.2-1B-Instruct-gptq-4bit\"\n",
    "    # Load calibration / eval dataset texts\n",
    "    calibration_texts = load_dataset(\n",
    "        \"allenai/c4\",\n",
    "        data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "        split=\"train\"\n",
    "    ).select(range(1024))[\"text\"]\n",
    "\n",
    "    # Subset for evaluation\n",
    "    eval_texts = calibration_texts[:100]\n",
    "\n",
    "    # Try loading quantized model if exists\n",
    "    try:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        quant_model = load_quant_model(quant_path, device=device)\n",
    "        quant_exists = True\n",
    "    except Exception as e:\n",
    "        print(\"Quantized model not found or failed to load:\", e)\n",
    "        quant_exists = False\n",
    "\n",
    "    # Load normal (unquantized) model\n",
    "    normal_model = load_normal_model(model_id)\n",
    "\n",
    "    # If quantized model doesn't exist, create it\n",
    "    if not quant_exists:\n",
    "        quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "        print(\"Quantizing model now...\")\n",
    "        model = GPTQModel.load(model_id, quant_config)\n",
    "        model.quantize(calibration_texts, batch_size=1)\n",
    "        model.save(quant_path)\n",
    "        quant_model = load_quant_model(quant_path, device=device)\n",
    "\n",
    "    # Evaluate normal model\n",
    "    print(\"Evaluating normal model...\")\n",
    "    ppl_normal = calculate_avg_ppl(normal_model, normal_model.tokenizer, eval_texts)\n",
    "    print(\"Normal model avg PPL:\", ppl_normal)\n",
    "\n",
    "    # Evaluate quantized model\n",
    "    print(\"Evaluating quantized model...\")\n",
    "    ppl_quant = calculate_avg_ppl(quant_model, quant_model.tokenizer, eval_texts)\n",
    "    print(\"Quantized model avg PPL:\", ppl_quant)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ-Aqw1MaDIl"
   },
   "source": [
    "Allocated = currently in use.\n",
    "\n",
    "Reserved = currently held by PyTorch (in use + cached).\n",
    "\n",
    "Peak Allocated = highest ever “in use.”\n",
    "\n",
    "Peak Reserved = highest ever “held by cache.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743,
     "referenced_widgets": [
      "cea59f5d17c5440ca51bb348e3d28510",
      "0ebc27033c8241c68319bbde9ef7fd54",
      "2a9030bdfd634204b5f67cfdf8f8461a",
      "9e6aeab2d0b34099847ffa01b06a095a",
      "c6059f8125554b82b96cb2467e21015d",
      "0f72550bb0ba4a9fa7183228aaf4109c",
      "e87d5cedca774c02a95737e7405684b0",
      "f9c92ab49a7749b2b074891b4c3d6023",
      "0fbb7351c5c94fee8598c17f6db7cf72",
      "156d412af1d94502a4c06e8605347fcd",
      "cddc8e24b1134dfe98e7db4bb05f84cd"
     ]
    },
    "id": "4hJfT-n4-ln1",
    "outputId": "7716bc7f-de5a-436e-8a2b-e1f7e3768c80"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def measure_gpu_memory(model_loader_fn, *args, **kwargs):\n",
    "    # Reset stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_loader_fn(*args, **kwargs).to(device)\n",
    "\n",
    "    # Do a dummy forward pass to trigger memory usage (adjust input shape as per your model)\n",
    "    # For example, if it’s a language model:\n",
    "    dummy_input = torch.randint(0, 100, (1, 16)).to(device)\n",
    "    try:\n",
    "        _ = model.generate(dummy_input)\n",
    "    except Exception:\n",
    "        # fallback: try forward if generate not available\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Collect memory stats\n",
    "    mem_alloc = torch.cuda.memory_allocated(device)\n",
    "    mem_reserved = torch.cuda.memory_reserved(device)\n",
    "    peak_alloc = torch.cuda.max_memory_allocated(device)\n",
    "    peak_reserved = torch.cuda.max_memory_reserved(device)\n",
    "\n",
    "    print(f\"Memory allocated: {mem_alloc / (1024**2):.2f} MB\")\n",
    "    print(f\"Memory reserved:  {mem_reserved / (1024**2):.2f} MB\")\n",
    "    print(f\"Peak alloc:       {peak_alloc / (1024**2):.2f} MB\")\n",
    "    print(f\"Peak reserved:    {peak_reserved / (1024**2):.2f} MB\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_normal_model(model_id):\n",
    "    quantize_config=QuantizeConfig(bits=4, group_size=128)\n",
    "    return GPTQModel.load(model_id, quantize_config=quantize_config)\n",
    "#Example usage:\n",
    "measure_gpu_memory(lambda: load_normal_model(model_id))\n",
    "# measure_gpu_memory(lambda: load_quant_model(quant_path, device=\"cuda:0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ed_oWVdukce"
   },
   "outputs": [],
   "source": [
    "# gptqmodel is integrated into lm-eval >= v0.4.7\n",
    "!pip install lm-eval>=0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965,
     "referenced_widgets": [
      "3842f64574d14f2e9fd4aa68bf804a4a",
      "fb459f54fe2a45009f289a8837e37cb2",
      "0318aaad42eb4fb1870f5188b6924ef2",
      "d867e71b48f245f98adf180680dcd786",
      "4a687ffad3694fc6be9cd41d50cfea7f",
      "9875cf8b4f884298b0917b9131f3fdf8",
      "5f0b4de82b034568871bcecfff72218f",
      "eb100b471e49488da204419fc56d933c",
      "0665c2e0b2ca4b83b9da36c5a2664fc9",
      "fbd17d1873ea4faf93905f2f2fe6c339",
      "8d52b9fa1e2f497c80aa83be988bacd6",
      "f2cea25da3554238afe42b1b8c2d9036",
      "af732f2d3b3840d3b91557c06bf967a6",
      "0819b4dc60c8416194fffe9d14e1f6ac",
      "bd9b9ccaebad4b5c880de0d237896a1c",
      "30a8ce782c6544d08b7efdaf8daef07d",
      "93f5aaa54b89497d9c31bd46dd3cfeee",
      "0b31de9d72e646dcaeb94962aca18cf7",
      "9767f0082373465e8c1fabd19cf337c8",
      "f80f4924df7f4795a215ab948f149d1d",
      "7e01399ca8594c82945c8ce6c616866f",
      "25a0a35d5f1649e2b680ee7de19c41be",
      "65e6aa2eab8e4d979e59e886237abfbc",
      "4727504b43e74aed961323257f42fd29",
      "3cd10ef4685843b89ff8a216bf5ff999",
      "44828519a95c42a9b3359c2979587903",
      "90108bd81b764f00adc36e1dc51cbd1e",
      "9ea5445fd88d4d2c84b5a03fe03ad486",
      "f971eedac5aa450fa1f5686c3d9ec123",
      "b7a4100e88734be19b2b7b1199193fcd",
      "b5d970595cb3498787479a40f2ebf382",
      "4319d1dbf869493fbd98e6ccb5efee7b",
      "ced23155ec594a928cc4c3d414919b65",
      "91cbc72967be4534bb22e235040d090b",
      "a1eb4c92a2814dd992901207120ba80c",
      "0afc37c45e0441738734402d3aef3dc0",
      "1b6cd92c888d49e6ab39e0d5634c0eff",
      "a2a8f596165542b49b34cdf6b3c0bafc",
      "f4e8d878954749cf941c158c6761dc58",
      "d3053309b1c64a28a878b94942fe1815",
      "28c1d1424b3d4d4598bca0d9b8adeb54",
      "c223f9b498fd485a810b646b6cb95ed3",
      "f36cde45f2ef4eb9a2edbed925038fea",
      "83bdf97a018b4d738e4fea65731ba7b2",
      "503c6bdf4e71412c9351a45f943e6b11",
      "38d178901e414471a6f2441b1da904ee",
      "f6cf30fccaff45c5abe491d2653f6e81",
      "8fa12d4911c04677a7772ae0acfb9526",
      "6b5056da774c45d494516adaeb4e88d7",
      "cf3a370aaf1c4078bdef210221fe8eb9",
      "8350e218113c4015abb377a467294f47",
      "c137aa0671594f239e97f8b42ef39d4a",
      "3ada4b46e62c437383610ffd4e7658ea",
      "4bd59eccdfa64e52a08830131e41ea3e",
      "8e17255b74724be1a55b47659ff7012f",
      "2d2d561c52a24284891c33b9ea010aca",
      "8335846c3f7e4259ad91bf8e651b66e2",
      "c69d46d195234672b1e5d59d4ca816e1",
      "06f5f7bfa48b4cd3aa710d7cfa876779",
      "a3a8fb851fe84a8eb4fbd9bd86f5db5e",
      "1b5e95944a2446c5bd40055cc9ea4f38",
      "02f0ea3f5f99443e9153a0c53f00499a",
      "befdd015378d4d609b4fc5dbf2865b72",
      "8b3f87af89e24275929a281a1b1a8c76",
      "9c74988a5dc048adada03b553fc5dd31",
      "ab45c3c2f06a4d17b34ea7f9eb6b7028",
      "7d8debbf623f4b6c8469eb7b7c9af60d",
      "0c10a241e73b469090854e5dcd8d0765",
      "7fdbc7dd670b4e2baf20e4737e41cd54",
      "6a8b34647b3642b2aad89e4207a9dc73",
      "83f015c6b4ba41e694ff5305f817be17",
      "f3d9b4421d68450d993620b38cb6aa8e",
      "b66ba9dcd764446ebb2fce560f1602a2",
      "b4f4e2d69dac4e3c9f99526e29bbd0f8",
      "d59539d2088f4ea1bc4594947b781527",
      "a468cbbf95c0464e821e5c96f7c64f8e",
      "442d3327874a4a64a6e8bce796442546"
     ]
    },
    "id": "bBdZTXo1vt92",
    "outputId": "0feca419-78fa-4106-c94b-a84ca736124c"
   },
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel\n",
    "from gptqmodel.utils.eval import EVAL\n",
    "\n",
    "model_id = \"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v1\"\n",
    "\n",
    "# Use `lm-eval` as framework to evaluate the model\n",
    "lm_eval_results = GPTQModel.eval(model_id, framework=EVAL.LM_EVAL, tasks=[EVAL.LM_EVAL.ARC_CHALLENGE])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIw9DHE1wNc"
   },
   "source": [
    "These are the evaluation metrics for the quantized model on the ARC Challenge task.\n",
    "\n",
    "acc is accuracy, acc_norm is normalized accuracy, etc.\n",
    "\n",
    "0.2799 means ~27.99% accuracy, with a standard error ±0.0131.\n",
    "\n",
    "The evaluation harness printed the result table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZLzxJLR2rqL"
   },
   "source": [
    "### Bits & Bytes / Quantization Context\n",
    "\n",
    "bitsandbytes is a library that implements efficient quantized operations (4-bit, 8-bit) for matrices / linear layers, especially useful for large models.\n",
    "\n",
    "The transformers library supports integration with bitsandbytes via a BitsAndBytesConfig class. That config tells from_pretrained how to load model weights in lower precision (4-bit or 8-bit) rather than default high precision.\n",
    "\n",
    "This integration is meant for inference (or adapter training) rather than full training from scratch. It reduces VRAM / memory footprint a lot, and you can still generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUUfFtFcsoCS"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate\n",
    "!pip install -U bitsandbytese\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"   # example HF model\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True)      # or load_in_8bit=True\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "inputs = tok(\"Explain GPTQ vs AWQ simply.\", return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=80)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQRA1anw2WJJ"
   },
   "source": [
    "### ExLlama\n",
    "\n",
    "ExLlama is a Python/C++/CUDA implementation tailored for Llama (and Llama-style) models, working especially with 4-bit GPTQ weights.\n",
    "\n",
    "It provides optimized kernels (linear layers, quantized operators) for inference, to accelerate generation speed and reduce memory overhead.\n",
    "\n",
    "ExLlamaV2 adds more features: e.g. EXL2 format (a flexible quantization format), better kernel optimizations, dynamic batching, new generation strategies.\n",
    "\n",
    "ExLlama is often integrated into quantization+inference pipelines — e.g. Transformers’ GPTQ support may allow using ExLlama kernels by specifying certain config flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9pnS9kiswif"
   },
   "outputs": [],
   "source": [
    "!pip install exllamav2\n",
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer, ExLlamaV2Generator\n",
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer, ExLlamaV2Generator\n",
    "# Download a GPTQ/EXL2 model first or point to HF snapshot dir with config + shards\n",
    "# Example assumes local path `./model` with GPTQ weights (ex: TheBloke GPTQ)\n",
    "model_path = \"/content/model\"  # put your GPTQ model dir here\n",
    "\n",
    "cfg = ExLlamaV2Config()\n",
    "cfg.model_dir = model_path\n",
    "model = ExLlamaV2(cfg)\n",
    "tok = ExLlamaV2Tokenizer(model_path)\n",
    "gen = ExLlamaV2Generator(model, tok)\n",
    "\n",
    "print(gen.generate_simple(\"Explain GPTQ vs AWQ simply.\", max_new_tokens=80))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
